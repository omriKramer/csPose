{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "\n",
    "import pose\n",
    "import models.cs_v2 as cs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('../../LIP').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentInstructor(cs.BaseInstructor):\n",
    "    def __init__(self, repeats):\n",
    "        self.repeats = repeats\n",
    "        \n",
    "    def on_batch_begin(self, **kwargs):\n",
    "        self.i = 0\n",
    "    \n",
    "    def next_inst(self, last_bu):\n",
    "        self.i += 1\n",
    "        state = {'continue': self.i < self.repeats}\n",
    "        return None, state\n",
    "    \n",
    "class RecurrentLoss:\n",
    "    \n",
    "    def __init__(self, repeats):\n",
    "        self.r = repeats\n",
    "        \n",
    "    def __call__(self, outputs, targets):\n",
    "        targets = targets.repeat(1, self.r, 1)\n",
    "#         total_loss = sum(pose.pose_ce_loss(phase_out, targets) for phase_out in outputs[1].split(16, dim=1))\n",
    "        return pose.pose_ce_loss(outputs[1], targets)\n",
    "    \n",
    "       \n",
    "pckh = partial(pose.Pckh, heatmap_func=lambda last_output: last_output[1][:, -16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (29866 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (8/16) (128, 128),Pose (7/16) (128, 128),Pose (6/16) (128, 128),Pose (14/16) (128, 128),Pose (1/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (14/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pose.get_data(root, 128)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor = RecurrentInstructor(2)\n",
    "learner = cs.cs_learner(data, models.resnet18, instructor, td_c=16, pretrained=False, embedding=None,\n",
    "                        loss_func=RecurrentLoss(2), callback_fns=pckh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='466', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      21.24% [99/466 01:45<06:30 18.1846]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.91E-02\n",
      "Min loss divided by 10: 2.75E-01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.390033</td>\n",
       "      <td>10.038353</td>\n",
       "      <td>0.623686</td>\n",
       "      <td>0.429158</td>\n",
       "      <td>0.249887</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.253393</td>\n",
       "      <td>0.237244</td>\n",
       "      <td>0.374306</td>\n",
       "      <td>0.388291</td>\n",
       "      <td>0.347454</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.319100</td>\n",
       "      <td>8.602507</td>\n",
       "      <td>0.723264</td>\n",
       "      <td>0.561116</td>\n",
       "      <td>0.376698</td>\n",
       "      <td>0.303917</td>\n",
       "      <td>0.363959</td>\n",
       "      <td>0.329186</td>\n",
       "      <td>0.403907</td>\n",
       "      <td>0.496221</td>\n",
       "      <td>0.445381</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.798311</td>\n",
       "      <td>8.063608</td>\n",
       "      <td>0.798218</td>\n",
       "      <td>0.630845</td>\n",
       "      <td>0.430555</td>\n",
       "      <td>0.371687</td>\n",
       "      <td>0.410116</td>\n",
       "      <td>0.385682</td>\n",
       "      <td>0.445319</td>\n",
       "      <td>0.562912</td>\n",
       "      <td>0.505005</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.461933</td>\n",
       "      <td>8.302901</td>\n",
       "      <td>0.795642</td>\n",
       "      <td>0.648359</td>\n",
       "      <td>0.453876</td>\n",
       "      <td>0.420115</td>\n",
       "      <td>0.395219</td>\n",
       "      <td>0.397264</td>\n",
       "      <td>0.447897</td>\n",
       "      <td>0.584159</td>\n",
       "      <td>0.517583</td>\n",
       "      <td>03:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.088418</td>\n",
       "      <td>538759790592.000000</td>\n",
       "      <td>0.798836</td>\n",
       "      <td>0.664653</td>\n",
       "      <td>0.529987</td>\n",
       "      <td>0.476533</td>\n",
       "      <td>0.471231</td>\n",
       "      <td>0.459719</td>\n",
       "      <td>0.474839</td>\n",
       "      <td>0.621338</td>\n",
       "      <td>0.562694</td>\n",
       "      <td>03:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.892107</td>\n",
       "      <td>6227011614008054441329033216.000000</td>\n",
       "      <td>0.824696</td>\n",
       "      <td>0.684965</td>\n",
       "      <td>0.548817</td>\n",
       "      <td>0.494874</td>\n",
       "      <td>0.504356</td>\n",
       "      <td>0.485719</td>\n",
       "      <td>0.497642</td>\n",
       "      <td>0.642237</td>\n",
       "      <td>0.586279</td>\n",
       "      <td>03:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.645669</td>\n",
       "      <td>69626793374654743904058867712.000000</td>\n",
       "      <td>0.832784</td>\n",
       "      <td>0.724218</td>\n",
       "      <td>0.584917</td>\n",
       "      <td>0.525864</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.515474</td>\n",
       "      <td>0.670749</td>\n",
       "      <td>0.611658</td>\n",
       "      <td>03:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.526010</td>\n",
       "      <td>210833.140625</td>\n",
       "      <td>0.841335</td>\n",
       "      <td>0.717900</td>\n",
       "      <td>0.584587</td>\n",
       "      <td>0.543428</td>\n",
       "      <td>0.498404</td>\n",
       "      <td>0.533776</td>\n",
       "      <td>0.524946</td>\n",
       "      <td>0.675503</td>\n",
       "      <td>0.614852</td>\n",
       "      <td>03:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.447293</td>\n",
       "      <td>8945909371095339711332352.000000</td>\n",
       "      <td>0.843653</td>\n",
       "      <td>0.746261</td>\n",
       "      <td>0.621611</td>\n",
       "      <td>0.566256</td>\n",
       "      <td>0.538234</td>\n",
       "      <td>0.555153</td>\n",
       "      <td>0.560555</td>\n",
       "      <td>0.697858</td>\n",
       "      <td>0.641137</td>\n",
       "      <td>04:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.288130</td>\n",
       "      <td>6.771254</td>\n",
       "      <td>0.861632</td>\n",
       "      <td>0.748278</td>\n",
       "      <td>0.631807</td>\n",
       "      <td>0.586542</td>\n",
       "      <td>0.544544</td>\n",
       "      <td>0.572399</td>\n",
       "      <td>0.561430</td>\n",
       "      <td>0.710371</td>\n",
       "      <td>0.652069</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.156525</td>\n",
       "      <td>6.528483</td>\n",
       "      <td>0.872089</td>\n",
       "      <td>0.766764</td>\n",
       "      <td>0.656975</td>\n",
       "      <td>0.606268</td>\n",
       "      <td>0.573101</td>\n",
       "      <td>0.595935</td>\n",
       "      <td>0.594041</td>\n",
       "      <td>0.728720</td>\n",
       "      <td>0.674147</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.072055</td>\n",
       "      <td>6.531141</td>\n",
       "      <td>0.871059</td>\n",
       "      <td>0.774207</td>\n",
       "      <td>0.664232</td>\n",
       "      <td>0.621263</td>\n",
       "      <td>0.577169</td>\n",
       "      <td>0.581820</td>\n",
       "      <td>0.573393</td>\n",
       "      <td>0.735758</td>\n",
       "      <td>0.675110</td>\n",
       "      <td>03:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.913029</td>\n",
       "      <td>47156.550781</td>\n",
       "      <td>0.867762</td>\n",
       "      <td>0.785505</td>\n",
       "      <td>0.674250</td>\n",
       "      <td>0.623774</td>\n",
       "      <td>0.589462</td>\n",
       "      <td>0.610315</td>\n",
       "      <td>0.593536</td>\n",
       "      <td>0.740859</td>\n",
       "      <td>0.685844</td>\n",
       "      <td>04:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.873579</td>\n",
       "      <td>6.381232</td>\n",
       "      <td>0.878426</td>\n",
       "      <td>0.788690</td>\n",
       "      <td>0.688564</td>\n",
       "      <td>0.646612</td>\n",
       "      <td>0.594587</td>\n",
       "      <td>0.616235</td>\n",
       "      <td>0.602105</td>\n",
       "      <td>0.753385</td>\n",
       "      <td>0.696003</td>\n",
       "      <td>03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6.779446</td>\n",
       "      <td>6.641374</td>\n",
       "      <td>0.864259</td>\n",
       "      <td>0.779039</td>\n",
       "      <td>0.670359</td>\n",
       "      <td>0.625800</td>\n",
       "      <td>0.587319</td>\n",
       "      <td>0.620376</td>\n",
       "      <td>0.615009</td>\n",
       "      <td>0.737801</td>\n",
       "      <td>0.687203</td>\n",
       "      <td>03:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.673386</td>\n",
       "      <td>6.198699</td>\n",
       "      <td>0.884195</td>\n",
       "      <td>0.805830</td>\n",
       "      <td>0.697355</td>\n",
       "      <td>0.643873</td>\n",
       "      <td>0.606552</td>\n",
       "      <td>0.642851</td>\n",
       "      <td>0.622464</td>\n",
       "      <td>0.760757</td>\n",
       "      <td>0.707873</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>6.672906</td>\n",
       "      <td>6.220998</td>\n",
       "      <td>0.886410</td>\n",
       "      <td>0.804410</td>\n",
       "      <td>0.710246</td>\n",
       "      <td>0.667150</td>\n",
       "      <td>0.615587</td>\n",
       "      <td>0.645376</td>\n",
       "      <td>0.633372</td>\n",
       "      <td>0.769758</td>\n",
       "      <td>0.716278</td>\n",
       "      <td>03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.513488</td>\n",
       "      <td>6.076714</td>\n",
       "      <td>0.889398</td>\n",
       "      <td>0.816164</td>\n",
       "      <td>0.720558</td>\n",
       "      <td>0.665025</td>\n",
       "      <td>0.621436</td>\n",
       "      <td>0.647797</td>\n",
       "      <td>0.647526</td>\n",
       "      <td>0.775540</td>\n",
       "      <td>0.722542</td>\n",
       "      <td>03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.486377</td>\n",
       "      <td>6.019044</td>\n",
       "      <td>0.899959</td>\n",
       "      <td>0.816942</td>\n",
       "      <td>0.729733</td>\n",
       "      <td>0.675767</td>\n",
       "      <td>0.624867</td>\n",
       "      <td>0.664374</td>\n",
       "      <td>0.658081</td>\n",
       "      <td>0.783286</td>\n",
       "      <td>0.731046</td>\n",
       "      <td>04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.396463</td>\n",
       "      <td>6.045494</td>\n",
       "      <td>0.893004</td>\n",
       "      <td>0.816683</td>\n",
       "      <td>0.729672</td>\n",
       "      <td>0.679652</td>\n",
       "      <td>0.627565</td>\n",
       "      <td>0.668839</td>\n",
       "      <td>0.653244</td>\n",
       "      <td>0.782311</td>\n",
       "      <td>0.730890</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.341413</td>\n",
       "      <td>5.973809</td>\n",
       "      <td>0.890995</td>\n",
       "      <td>0.824721</td>\n",
       "      <td>0.723376</td>\n",
       "      <td>0.687315</td>\n",
       "      <td>0.633876</td>\n",
       "      <td>0.673928</td>\n",
       "      <td>0.668961</td>\n",
       "      <td>0.784154</td>\n",
       "      <td>0.735302</td>\n",
       "      <td>07:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>6.346982</td>\n",
       "      <td>5.981849</td>\n",
       "      <td>0.895838</td>\n",
       "      <td>0.819407</td>\n",
       "      <td>0.730053</td>\n",
       "      <td>0.681782</td>\n",
       "      <td>0.625910</td>\n",
       "      <td>0.673232</td>\n",
       "      <td>0.659951</td>\n",
       "      <td>0.784327</td>\n",
       "      <td>0.733153</td>\n",
       "      <td>05:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>6.216465</td>\n",
       "      <td>5.879062</td>\n",
       "      <td>0.897847</td>\n",
       "      <td>0.832292</td>\n",
       "      <td>0.746067</td>\n",
       "      <td>0.700511</td>\n",
       "      <td>0.637682</td>\n",
       "      <td>0.686749</td>\n",
       "      <td>0.674428</td>\n",
       "      <td>0.796587</td>\n",
       "      <td>0.745740</td>\n",
       "      <td>04:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>6.182761</td>\n",
       "      <td>5.846845</td>\n",
       "      <td>0.902122</td>\n",
       "      <td>0.836201</td>\n",
       "      <td>0.746626</td>\n",
       "      <td>0.703929</td>\n",
       "      <td>0.644458</td>\n",
       "      <td>0.692118</td>\n",
       "      <td>0.686492</td>\n",
       "      <td>0.799658</td>\n",
       "      <td>0.750621</td>\n",
       "      <td>06:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6.097449</td>\n",
       "      <td>5.828505</td>\n",
       "      <td>0.896868</td>\n",
       "      <td>0.840856</td>\n",
       "      <td>0.754879</td>\n",
       "      <td>0.711497</td>\n",
       "      <td>0.649639</td>\n",
       "      <td>0.697979</td>\n",
       "      <td>0.701645</td>\n",
       "      <td>0.803317</td>\n",
       "      <td>0.756013</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>6.037210</td>\n",
       "      <td>5.776432</td>\n",
       "      <td>0.904956</td>\n",
       "      <td>0.842263</td>\n",
       "      <td>0.761065</td>\n",
       "      <td>0.712250</td>\n",
       "      <td>0.643737</td>\n",
       "      <td>0.696141</td>\n",
       "      <td>0.689153</td>\n",
       "      <td>0.807470</td>\n",
       "      <td>0.756120</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>5.991773</td>\n",
       "      <td>5.741285</td>\n",
       "      <td>0.907737</td>\n",
       "      <td>0.844414</td>\n",
       "      <td>0.762994</td>\n",
       "      <td>0.716298</td>\n",
       "      <td>0.650191</td>\n",
       "      <td>0.699088</td>\n",
       "      <td>0.699888</td>\n",
       "      <td>0.810181</td>\n",
       "      <td>0.760278</td>\n",
       "      <td>04:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>5.911313</td>\n",
       "      <td>5.714320</td>\n",
       "      <td>0.901453</td>\n",
       "      <td>0.848114</td>\n",
       "      <td>0.764229</td>\n",
       "      <td>0.723375</td>\n",
       "      <td>0.650681</td>\n",
       "      <td>0.711466</td>\n",
       "      <td>0.702882</td>\n",
       "      <td>0.811517</td>\n",
       "      <td>0.763052</td>\n",
       "      <td>04:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>5.874964</td>\n",
       "      <td>5.679445</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.850935</td>\n",
       "      <td>0.766337</td>\n",
       "      <td>0.726141</td>\n",
       "      <td>0.660211</td>\n",
       "      <td>0.713792</td>\n",
       "      <td>0.708237</td>\n",
       "      <td>0.814415</td>\n",
       "      <td>0.767151</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>5.778435</td>\n",
       "      <td>5.705392</td>\n",
       "      <td>0.906707</td>\n",
       "      <td>0.849524</td>\n",
       "      <td>0.769655</td>\n",
       "      <td>0.727202</td>\n",
       "      <td>0.658753</td>\n",
       "      <td>0.711977</td>\n",
       "      <td>0.699944</td>\n",
       "      <td>0.815483</td>\n",
       "      <td>0.766468</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.679926</td>\n",
       "      <td>5.617636</td>\n",
       "      <td>0.906553</td>\n",
       "      <td>0.855581</td>\n",
       "      <td>0.779489</td>\n",
       "      <td>0.735282</td>\n",
       "      <td>0.666773</td>\n",
       "      <td>0.722786</td>\n",
       "      <td>0.722023</td>\n",
       "      <td>0.821346</td>\n",
       "      <td>0.775013</td>\n",
       "      <td>04:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.664740</td>\n",
       "      <td>5.622315</td>\n",
       "      <td>0.909437</td>\n",
       "      <td>0.856053</td>\n",
       "      <td>0.780707</td>\n",
       "      <td>0.734890</td>\n",
       "      <td>0.668675</td>\n",
       "      <td>0.723233</td>\n",
       "      <td>0.723252</td>\n",
       "      <td>0.822414</td>\n",
       "      <td>0.776149</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.578750</td>\n",
       "      <td>5.610669</td>\n",
       "      <td>0.909386</td>\n",
       "      <td>0.856940</td>\n",
       "      <td>0.784657</td>\n",
       "      <td>0.739656</td>\n",
       "      <td>0.668625</td>\n",
       "      <td>0.727940</td>\n",
       "      <td>0.726102</td>\n",
       "      <td>0.824738</td>\n",
       "      <td>0.778470</td>\n",
       "      <td>03:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>5.513852</td>\n",
       "      <td>5.592869</td>\n",
       "      <td>0.909231</td>\n",
       "      <td>0.860699</td>\n",
       "      <td>0.783295</td>\n",
       "      <td>0.740017</td>\n",
       "      <td>0.672773</td>\n",
       "      <td>0.731415</td>\n",
       "      <td>0.723806</td>\n",
       "      <td>0.825419</td>\n",
       "      <td>0.779705</td>\n",
       "      <td>03:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>5.475200</td>\n",
       "      <td>5.575933</td>\n",
       "      <td>0.913507</td>\n",
       "      <td>0.864406</td>\n",
       "      <td>0.783210</td>\n",
       "      <td>0.743844</td>\n",
       "      <td>0.673643</td>\n",
       "      <td>0.732598</td>\n",
       "      <td>0.728566</td>\n",
       "      <td>0.828357</td>\n",
       "      <td>0.782306</td>\n",
       "      <td>05:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.419571</td>\n",
       "      <td>5.579405</td>\n",
       "      <td>0.910622</td>\n",
       "      <td>0.862633</td>\n",
       "      <td>0.787176</td>\n",
       "      <td>0.743601</td>\n",
       "      <td>0.671957</td>\n",
       "      <td>0.734894</td>\n",
       "      <td>0.730564</td>\n",
       "      <td>0.828076</td>\n",
       "      <td>0.782380</td>\n",
       "      <td>05:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.379198</td>\n",
       "      <td>5.579348</td>\n",
       "      <td>0.911395</td>\n",
       "      <td>0.862425</td>\n",
       "      <td>0.786591</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>0.673266</td>\n",
       "      <td>0.735433</td>\n",
       "      <td>0.729107</td>\n",
       "      <td>0.828650</td>\n",
       "      <td>0.782841</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>5.362628</td>\n",
       "      <td>5.574671</td>\n",
       "      <td>0.911447</td>\n",
       "      <td>0.863102</td>\n",
       "      <td>0.787396</td>\n",
       "      <td>0.745746</td>\n",
       "      <td>0.675393</td>\n",
       "      <td>0.736464</td>\n",
       "      <td>0.732188</td>\n",
       "      <td>0.828971</td>\n",
       "      <td>0.783821</td>\n",
       "      <td>05:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>5.362766</td>\n",
       "      <td>5.568936</td>\n",
       "      <td>0.912889</td>\n",
       "      <td>0.863468</td>\n",
       "      <td>0.787495</td>\n",
       "      <td>0.746755</td>\n",
       "      <td>0.675123</td>\n",
       "      <td>0.736656</td>\n",
       "      <td>0.732879</td>\n",
       "      <td>0.829705</td>\n",
       "      <td>0.784331</td>\n",
       "      <td>07:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>5.377920</td>\n",
       "      <td>5.575194</td>\n",
       "      <td>0.912528</td>\n",
       "      <td>0.863677</td>\n",
       "      <td>0.786315</td>\n",
       "      <td>0.746464</td>\n",
       "      <td>0.675069</td>\n",
       "      <td>0.736598</td>\n",
       "      <td>0.732570</td>\n",
       "      <td>0.829305</td>\n",
       "      <td>0.784035</td>\n",
       "      <td>08:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(40, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('repeat2-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (29866 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (8/16) (128, 128),Pose (7/16) (128, 128),Pose (6/16) (128, 128),Pose (14/16) (128, 128),Pose (1/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (14/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Test: None, model=CounterStream(\n",
       "  (bu_body): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (td): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TDHead(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (laterals): ModuleList(\n",
       "    (0): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (12): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (13): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (14): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (15): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<__main__.RecurrentLoss object at 0x7ff04d15c4a8>, metrics=[], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/labs/waic/omrik/LIP'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'pose.Pckh'>, heatmap_func=<function <lambda> at 0x7fef89842d90>)], callbacks=[RecurrentInstructor\n",
       "repeats: 2], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Lambda()\n",
       "  (17): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (28): ReLU(inplace=True)\n",
       "  (29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (30): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (33): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (34): ReLU(inplace=True)\n",
       "  (35): Lambda()\n",
       "  (36): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (39): ReLU(inplace=True)\n",
       "  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (42): ReLU(inplace=True)\n",
       "  (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (45): ReLU(inplace=True)\n",
       "  (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (49): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Lambda()\n",
       "  (55): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (56): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (58): ReLU(inplace=True)\n",
       "  (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (61): ReLU(inplace=True)\n",
       "  (62): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (64): ReLU(inplace=True)\n",
       "  (65): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (66): ReLU(inplace=True)\n",
       "  (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (69): ReLU(inplace=True)\n",
       "  (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (72): ReLU(inplace=True)\n",
       "  (73): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (76): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (77): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (78): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (79): ReLU(inplace=True)\n",
       "  (80): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (81): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (82): ReLU(inplace=True)\n",
       "  (83): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (84): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (85): ReLU(inplace=True)\n",
       "  (86): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (87): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (88): ReLU(inplace=True)\n",
       "  (89): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (90): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (91): ReLU(inplace=True)\n",
       "  (92): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (93): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (94): ReLU(inplace=True)\n",
       "  (95): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (96): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (97): ReLU(inplace=True)\n",
       "  (98): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (99): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (100): ReLU(inplace=True)\n",
       "  (101): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (102): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (103): ReLU(inplace=True)\n",
       "  (104): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (105): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (106): ReLU(inplace=True)\n",
       "  (107): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (108): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (109): ReLU(inplace=True)\n",
       "  (110): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (111): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (112): ReLU(inplace=True)\n",
       "  (113): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (114): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (115): ReLU(inplace=True)\n",
       "  (116): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (117): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (118): ReLU(inplace=True)\n",
       "  (119): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (120): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (121): ReLU(inplace=True)\n",
       "  (122): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (123): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (124): ReLU(inplace=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('repeat2-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data = pose.get_data(root, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.926105</td>\n",
       "      <td>4.395753</td>\n",
       "      <td>0.897589</td>\n",
       "      <td>0.837308</td>\n",
       "      <td>0.740839</td>\n",
       "      <td>0.692453</td>\n",
       "      <td>0.564051</td>\n",
       "      <td>0.567554</td>\n",
       "      <td>0.521076</td>\n",
       "      <td>0.794624</td>\n",
       "      <td>0.702020</td>\n",
       "      <td>09:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.649168</td>\n",
       "      <td>4.211531</td>\n",
       "      <td>0.905986</td>\n",
       "      <td>0.850052</td>\n",
       "      <td>0.764899</td>\n",
       "      <td>0.722162</td>\n",
       "      <td>0.592745</td>\n",
       "      <td>0.618238</td>\n",
       "      <td>0.571703</td>\n",
       "      <td>0.813066</td>\n",
       "      <td>0.729507</td>\n",
       "      <td>07:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.518838</td>\n",
       "      <td>4.140831</td>\n",
       "      <td>0.910674</td>\n",
       "      <td>0.856839</td>\n",
       "      <td>0.778697</td>\n",
       "      <td>0.740333</td>\n",
       "      <td>0.616804</td>\n",
       "      <td>0.656937</td>\n",
       "      <td>0.606793</td>\n",
       "      <td>0.823749</td>\n",
       "      <td>0.748341</td>\n",
       "      <td>08:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.423072</td>\n",
       "      <td>4.043563</td>\n",
       "      <td>0.915516</td>\n",
       "      <td>0.863629</td>\n",
       "      <td>0.789993</td>\n",
       "      <td>0.752757</td>\n",
       "      <td>0.635457</td>\n",
       "      <td>0.684560</td>\n",
       "      <td>0.647863</td>\n",
       "      <td>0.832483</td>\n",
       "      <td>0.764410</td>\n",
       "      <td>09:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.338748</td>\n",
       "      <td>3.985364</td>\n",
       "      <td>0.919843</td>\n",
       "      <td>0.868587</td>\n",
       "      <td>0.798230</td>\n",
       "      <td>0.764237</td>\n",
       "      <td>0.650954</td>\n",
       "      <td>0.706227</td>\n",
       "      <td>0.683412</td>\n",
       "      <td>0.839641</td>\n",
       "      <td>0.777688</td>\n",
       "      <td>08:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.274280</td>\n",
       "      <td>3.950999</td>\n",
       "      <td>0.921904</td>\n",
       "      <td>0.871511</td>\n",
       "      <td>0.805345</td>\n",
       "      <td>0.769022</td>\n",
       "      <td>0.662181</td>\n",
       "      <td>0.719391</td>\n",
       "      <td>0.701801</td>\n",
       "      <td>0.843821</td>\n",
       "      <td>0.785583</td>\n",
       "      <td>09:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.228867</td>\n",
       "      <td>3.909516</td>\n",
       "      <td>0.924222</td>\n",
       "      <td>0.876472</td>\n",
       "      <td>0.811345</td>\n",
       "      <td>0.776067</td>\n",
       "      <td>0.673491</td>\n",
       "      <td>0.732366</td>\n",
       "      <td>0.715570</td>\n",
       "      <td>0.848842</td>\n",
       "      <td>0.793485</td>\n",
       "      <td>08:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.181600</td>\n",
       "      <td>3.907747</td>\n",
       "      <td>0.923862</td>\n",
       "      <td>0.877461</td>\n",
       "      <td>0.810433</td>\n",
       "      <td>0.778338</td>\n",
       "      <td>0.675236</td>\n",
       "      <td>0.737341</td>\n",
       "      <td>0.722566</td>\n",
       "      <td>0.849323</td>\n",
       "      <td>0.795420</td>\n",
       "      <td>08:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.162936</td>\n",
       "      <td>3.964754</td>\n",
       "      <td>0.921337</td>\n",
       "      <td>0.873546</td>\n",
       "      <td>0.803343</td>\n",
       "      <td>0.771275</td>\n",
       "      <td>0.664253</td>\n",
       "      <td>0.727458</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.844248</td>\n",
       "      <td>0.789205</td>\n",
       "      <td>07:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.137141</td>\n",
       "      <td>3.895944</td>\n",
       "      <td>0.925613</td>\n",
       "      <td>0.880434</td>\n",
       "      <td>0.817741</td>\n",
       "      <td>0.783659</td>\n",
       "      <td>0.678999</td>\n",
       "      <td>0.741137</td>\n",
       "      <td>0.736731</td>\n",
       "      <td>0.853596</td>\n",
       "      <td>0.800614</td>\n",
       "      <td>09:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.120608</td>\n",
       "      <td>3.856570</td>\n",
       "      <td>0.924531</td>\n",
       "      <td>0.882735</td>\n",
       "      <td>0.820269</td>\n",
       "      <td>0.785485</td>\n",
       "      <td>0.687721</td>\n",
       "      <td>0.751621</td>\n",
       "      <td>0.745810</td>\n",
       "      <td>0.854972</td>\n",
       "      <td>0.805068</td>\n",
       "      <td>10:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.081258</td>\n",
       "      <td>3.877501</td>\n",
       "      <td>0.925716</td>\n",
       "      <td>0.882368</td>\n",
       "      <td>0.820529</td>\n",
       "      <td>0.785565</td>\n",
       "      <td>0.685543</td>\n",
       "      <td>0.751936</td>\n",
       "      <td>0.738660</td>\n",
       "      <td>0.855252</td>\n",
       "      <td>0.804187</td>\n",
       "      <td>09:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.067413</td>\n",
       "      <td>3.849206</td>\n",
       "      <td>0.927519</td>\n",
       "      <td>0.886701</td>\n",
       "      <td>0.826783</td>\n",
       "      <td>0.788851</td>\n",
       "      <td>0.694271</td>\n",
       "      <td>0.756579</td>\n",
       "      <td>0.747805</td>\n",
       "      <td>0.859165</td>\n",
       "      <td>0.809480</td>\n",
       "      <td>10:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.025631</td>\n",
       "      <td>3.841501</td>\n",
       "      <td>0.928395</td>\n",
       "      <td>0.885185</td>\n",
       "      <td>0.826318</td>\n",
       "      <td>0.790169</td>\n",
       "      <td>0.687387</td>\n",
       "      <td>0.754951</td>\n",
       "      <td>0.750195</td>\n",
       "      <td>0.859205</td>\n",
       "      <td>0.808517</td>\n",
       "      <td>09:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.001565</td>\n",
       "      <td>3.848416</td>\n",
       "      <td>0.928446</td>\n",
       "      <td>0.887119</td>\n",
       "      <td>0.829818</td>\n",
       "      <td>0.790458</td>\n",
       "      <td>0.684336</td>\n",
       "      <td>0.761504</td>\n",
       "      <td>0.750949</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>0.809842</td>\n",
       "      <td>09:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.017488</td>\n",
       "      <td>3.848565</td>\n",
       "      <td>0.925716</td>\n",
       "      <td>0.886751</td>\n",
       "      <td>0.829653</td>\n",
       "      <td>0.789229</td>\n",
       "      <td>0.687896</td>\n",
       "      <td>0.761119</td>\n",
       "      <td>0.754051</td>\n",
       "      <td>0.859499</td>\n",
       "      <td>0.809957</td>\n",
       "      <td>09:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.996072</td>\n",
       "      <td>3.839593</td>\n",
       "      <td>0.927210</td>\n",
       "      <td>0.886906</td>\n",
       "      <td>0.829537</td>\n",
       "      <td>0.793838</td>\n",
       "      <td>0.696618</td>\n",
       "      <td>0.758883</td>\n",
       "      <td>0.755722</td>\n",
       "      <td>0.861008</td>\n",
       "      <td>0.812106</td>\n",
       "      <td>08:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.948322</td>\n",
       "      <td>3.829164</td>\n",
       "      <td>0.928034</td>\n",
       "      <td>0.887587</td>\n",
       "      <td>0.827304</td>\n",
       "      <td>0.791319</td>\n",
       "      <td>0.691549</td>\n",
       "      <td>0.760779</td>\n",
       "      <td>0.754030</td>\n",
       "      <td>0.860247</td>\n",
       "      <td>0.810929</td>\n",
       "      <td>09:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.956187</td>\n",
       "      <td>3.808652</td>\n",
       "      <td>0.929528</td>\n",
       "      <td>0.889728</td>\n",
       "      <td>0.831908</td>\n",
       "      <td>0.800084</td>\n",
       "      <td>0.701163</td>\n",
       "      <td>0.766684</td>\n",
       "      <td>0.761959</td>\n",
       "      <td>0.864413</td>\n",
       "      <td>0.816535</td>\n",
       "      <td>08:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.935442</td>\n",
       "      <td>3.841774</td>\n",
       "      <td>0.929683</td>\n",
       "      <td>0.885709</td>\n",
       "      <td>0.829524</td>\n",
       "      <td>0.793008</td>\n",
       "      <td>0.693407</td>\n",
       "      <td>0.759002</td>\n",
       "      <td>0.760877</td>\n",
       "      <td>0.861142</td>\n",
       "      <td>0.812271</td>\n",
       "      <td>08:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.914210</td>\n",
       "      <td>3.836538</td>\n",
       "      <td>0.927674</td>\n",
       "      <td>0.885864</td>\n",
       "      <td>0.829907</td>\n",
       "      <td>0.797433</td>\n",
       "      <td>0.697284</td>\n",
       "      <td>0.760652</td>\n",
       "      <td>0.761347</td>\n",
       "      <td>0.861809</td>\n",
       "      <td>0.813522</td>\n",
       "      <td>08:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.873662</td>\n",
       "      <td>3.804513</td>\n",
       "      <td>0.930455</td>\n",
       "      <td>0.889781</td>\n",
       "      <td>0.838096</td>\n",
       "      <td>0.799613</td>\n",
       "      <td>0.700061</td>\n",
       "      <td>0.771049</td>\n",
       "      <td>0.762190</td>\n",
       "      <td>0.866056</td>\n",
       "      <td>0.817959</td>\n",
       "      <td>07:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.903915</td>\n",
       "      <td>3.796425</td>\n",
       "      <td>0.930352</td>\n",
       "      <td>0.890198</td>\n",
       "      <td>0.833180</td>\n",
       "      <td>0.799097</td>\n",
       "      <td>0.699452</td>\n",
       "      <td>0.769619</td>\n",
       "      <td>0.767502</td>\n",
       "      <td>0.864814</td>\n",
       "      <td>0.817490</td>\n",
       "      <td>09:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.877362</td>\n",
       "      <td>3.825879</td>\n",
       "      <td>0.928189</td>\n",
       "      <td>0.890509</td>\n",
       "      <td>0.832121</td>\n",
       "      <td>0.797291</td>\n",
       "      <td>0.695799</td>\n",
       "      <td>0.763853</td>\n",
       "      <td>0.764038</td>\n",
       "      <td>0.863639</td>\n",
       "      <td>0.815119</td>\n",
       "      <td>09:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.870391</td>\n",
       "      <td>3.803057</td>\n",
       "      <td>0.930971</td>\n",
       "      <td>0.891084</td>\n",
       "      <td>0.835371</td>\n",
       "      <td>0.800213</td>\n",
       "      <td>0.699510</td>\n",
       "      <td>0.768239</td>\n",
       "      <td>0.768730</td>\n",
       "      <td>0.866003</td>\n",
       "      <td>0.818189</td>\n",
       "      <td>08:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.847356</td>\n",
       "      <td>3.798279</td>\n",
       "      <td>0.929477</td>\n",
       "      <td>0.890513</td>\n",
       "      <td>0.835513</td>\n",
       "      <td>0.800818</td>\n",
       "      <td>0.701695</td>\n",
       "      <td>0.774208</td>\n",
       "      <td>0.772120</td>\n",
       "      <td>0.865655</td>\n",
       "      <td>0.819416</td>\n",
       "      <td>10:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.821968</td>\n",
       "      <td>3.811470</td>\n",
       "      <td>0.931537</td>\n",
       "      <td>0.891659</td>\n",
       "      <td>0.836701</td>\n",
       "      <td>0.802129</td>\n",
       "      <td>0.701489</td>\n",
       "      <td>0.771591</td>\n",
       "      <td>0.770190</td>\n",
       "      <td>0.867071</td>\n",
       "      <td>0.819720</td>\n",
       "      <td>09:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.832701</td>\n",
       "      <td>3.818564</td>\n",
       "      <td>0.930713</td>\n",
       "      <td>0.891034</td>\n",
       "      <td>0.832944</td>\n",
       "      <td>0.801458</td>\n",
       "      <td>0.699729</td>\n",
       "      <td>0.770917</td>\n",
       "      <td>0.764183</td>\n",
       "      <td>0.865642</td>\n",
       "      <td>0.817852</td>\n",
       "      <td>07:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.819702</td>\n",
       "      <td>3.794498</td>\n",
       "      <td>0.930713</td>\n",
       "      <td>0.893019</td>\n",
       "      <td>0.834844</td>\n",
       "      <td>0.801882</td>\n",
       "      <td>0.700661</td>\n",
       "      <td>0.774658</td>\n",
       "      <td>0.772667</td>\n",
       "      <td>0.866697</td>\n",
       "      <td>0.820017</td>\n",
       "      <td>07:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.809272</td>\n",
       "      <td>3.812167</td>\n",
       "      <td>0.930713</td>\n",
       "      <td>0.891607</td>\n",
       "      <td>0.835110</td>\n",
       "      <td>0.802048</td>\n",
       "      <td>0.701429</td>\n",
       "      <td>0.769810</td>\n",
       "      <td>0.769034</td>\n",
       "      <td>0.866443</td>\n",
       "      <td>0.818980</td>\n",
       "      <td>08:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.798955</td>\n",
       "      <td>3.808474</td>\n",
       "      <td>0.931177</td>\n",
       "      <td>0.892547</td>\n",
       "      <td>0.837968</td>\n",
       "      <td>0.803519</td>\n",
       "      <td>0.702680</td>\n",
       "      <td>0.773348</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.867859</td>\n",
       "      <td>0.820856</td>\n",
       "      <td>08:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.792626</td>\n",
       "      <td>3.808040</td>\n",
       "      <td>0.930198</td>\n",
       "      <td>0.892286</td>\n",
       "      <td>0.837789</td>\n",
       "      <td>0.803839</td>\n",
       "      <td>0.700496</td>\n",
       "      <td>0.774858</td>\n",
       "      <td>0.772506</td>\n",
       "      <td>0.867565</td>\n",
       "      <td>0.820535</td>\n",
       "      <td>08:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.779091</td>\n",
       "      <td>3.810815</td>\n",
       "      <td>0.930198</td>\n",
       "      <td>0.892966</td>\n",
       "      <td>0.836312</td>\n",
       "      <td>0.804098</td>\n",
       "      <td>0.701747</td>\n",
       "      <td>0.774001</td>\n",
       "      <td>0.770727</td>\n",
       "      <td>0.867445</td>\n",
       "      <td>0.820354</td>\n",
       "      <td>07:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.794710</td>\n",
       "      <td>8.898967</td>\n",
       "      <td>0.927880</td>\n",
       "      <td>0.889991</td>\n",
       "      <td>0.835375</td>\n",
       "      <td>0.801555</td>\n",
       "      <td>0.700379</td>\n",
       "      <td>0.772433</td>\n",
       "      <td>0.771499</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.818675</td>\n",
       "      <td>09:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.769141</td>\n",
       "      <td>3.805000</td>\n",
       "      <td>0.930868</td>\n",
       "      <td>0.892704</td>\n",
       "      <td>0.837678</td>\n",
       "      <td>0.806462</td>\n",
       "      <td>0.703108</td>\n",
       "      <td>0.776434</td>\n",
       "      <td>0.773737</td>\n",
       "      <td>0.868446</td>\n",
       "      <td>0.821803</td>\n",
       "      <td>08:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.777163</td>\n",
       "      <td>3.834314</td>\n",
       "      <td>0.929631</td>\n",
       "      <td>0.891504</td>\n",
       "      <td>0.837350</td>\n",
       "      <td>0.804076</td>\n",
       "      <td>0.701969</td>\n",
       "      <td>0.774989</td>\n",
       "      <td>0.773430</td>\n",
       "      <td>0.867164</td>\n",
       "      <td>0.820626</td>\n",
       "      <td>07:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.752814</td>\n",
       "      <td>3.807621</td>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.893124</td>\n",
       "      <td>0.837781</td>\n",
       "      <td>0.806909</td>\n",
       "      <td>0.704206</td>\n",
       "      <td>0.776969</td>\n",
       "      <td>0.773342</td>\n",
       "      <td>0.868820</td>\n",
       "      <td>0.822223</td>\n",
       "      <td>06:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.773089</td>\n",
       "      <td>3.809611</td>\n",
       "      <td>0.930764</td>\n",
       "      <td>0.892758</td>\n",
       "      <td>0.838221</td>\n",
       "      <td>0.804736</td>\n",
       "      <td>0.702840</td>\n",
       "      <td>0.774073</td>\n",
       "      <td>0.772273</td>\n",
       "      <td>0.868153</td>\n",
       "      <td>0.821128</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.761230</td>\n",
       "      <td>3.812932</td>\n",
       "      <td>0.931331</td>\n",
       "      <td>0.893174</td>\n",
       "      <td>0.838552</td>\n",
       "      <td>0.806026</td>\n",
       "      <td>0.701860</td>\n",
       "      <td>0.774851</td>\n",
       "      <td>0.773503</td>\n",
       "      <td>0.868794</td>\n",
       "      <td>0.821606</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.771594</td>\n",
       "      <td>3.808515</td>\n",
       "      <td>0.931074</td>\n",
       "      <td>0.892548</td>\n",
       "      <td>0.837239</td>\n",
       "      <td>0.806412</td>\n",
       "      <td>0.703217</td>\n",
       "      <td>0.774397</td>\n",
       "      <td>0.773732</td>\n",
       "      <td>0.868340</td>\n",
       "      <td>0.821499</td>\n",
       "      <td>05:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(40 , 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('repeat2-256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/labs/waic/omrik/csPose/pose.py\u001b[0m(162)\u001b[0;36mpose_ce_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    160 \u001b[0;31m    \u001b[0mis_visible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    161 \u001b[0;31m    \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_visible\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 162 \u001b[0;31m    \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_visible\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    163 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    164 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> output.shape\n",
      "*** AttributeError: 'tuple' object has no attribute 'shape'\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
