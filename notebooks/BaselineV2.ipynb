{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "import sys\n",
    "\n",
    "import pose\n",
    "import models.cs_v2 as cs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (29866 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (8/16) (128, 128),Pose (7/16) (128, 128),Pose (6/16) (128, 128),Pose (14/16) (128, 128),Pose (1/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (14/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Path('../../LIP').resolve()\n",
    "\n",
    "def loss(outputs, targets):\n",
    "    return pose.pose_ce_loss(outputs[1], targets)\n",
    "\n",
    "data = pose.get_data(root, 128)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2716, 0.2401, 0.2315]), tensor([0.3093, 0.2853, 0.2856])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.batch_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2682, 0.2387, 0.2219]) tensor([0.3046, 0.2805, 0.2727])\n"
     ]
    }
   ],
   "source": [
    "mean, std = torch.zeros(3), torch.zeros(3)\n",
    "for image, target in data.dl(DatasetType.Train):\n",
    "    n = len(image)\n",
    "    image = channel_view(image)\n",
    "    mean += torch.mean(image, 1).cpu() * n\n",
    "    std += torch.std(image, 1).cpu() * n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2682, 0.2387, 0.2219]), tensor([0.3046, 0.2805, 0.2727]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lip_stats = mean / len(data.x), std / len(data.x)\n",
    "lip_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.986288</td>\n",
       "      <td>4.583123</td>\n",
       "      <td>0.698228</td>\n",
       "      <td>0.470461</td>\n",
       "      <td>0.307428</td>\n",
       "      <td>0.264155</td>\n",
       "      <td>0.312642</td>\n",
       "      <td>0.251317</td>\n",
       "      <td>0.360718</td>\n",
       "      <td>0.440026</td>\n",
       "      <td>0.388614</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.528166</td>\n",
       "      <td>4.134044</td>\n",
       "      <td>0.756130</td>\n",
       "      <td>0.578077</td>\n",
       "      <td>0.395379</td>\n",
       "      <td>0.352256</td>\n",
       "      <td>0.378554</td>\n",
       "      <td>0.355883</td>\n",
       "      <td>0.438474</td>\n",
       "      <td>0.525320</td>\n",
       "      <td>0.472588</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.295240</td>\n",
       "      <td>3.934727</td>\n",
       "      <td>0.802390</td>\n",
       "      <td>0.639839</td>\n",
       "      <td>0.451451</td>\n",
       "      <td>0.410891</td>\n",
       "      <td>0.420147</td>\n",
       "      <td>0.414812</td>\n",
       "      <td>0.440466</td>\n",
       "      <td>0.580901</td>\n",
       "      <td>0.520745</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.148882</td>\n",
       "      <td>3.809476</td>\n",
       "      <td>0.803730</td>\n",
       "      <td>0.659952</td>\n",
       "      <td>0.492608</td>\n",
       "      <td>0.441730</td>\n",
       "      <td>0.449711</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.463090</td>\n",
       "      <td>0.603950</td>\n",
       "      <td>0.545078</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.015841</td>\n",
       "      <td>3.661621</td>\n",
       "      <td>0.836802</td>\n",
       "      <td>0.688823</td>\n",
       "      <td>0.523399</td>\n",
       "      <td>0.482196</td>\n",
       "      <td>0.486048</td>\n",
       "      <td>0.474286</td>\n",
       "      <td>0.487915</td>\n",
       "      <td>0.637096</td>\n",
       "      <td>0.577882</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.901411</td>\n",
       "      <td>3.622176</td>\n",
       "      <td>0.824542</td>\n",
       "      <td>0.706932</td>\n",
       "      <td>0.558503</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.486552</td>\n",
       "      <td>0.485326</td>\n",
       "      <td>0.497452</td>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.587678</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.867282</td>\n",
       "      <td>3.508065</td>\n",
       "      <td>0.843190</td>\n",
       "      <td>0.718421</td>\n",
       "      <td>0.583790</td>\n",
       "      <td>0.528701</td>\n",
       "      <td>0.501219</td>\n",
       "      <td>0.516240</td>\n",
       "      <td>0.518320</td>\n",
       "      <td>0.672284</td>\n",
       "      <td>0.610382</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.780718</td>\n",
       "      <td>3.452451</td>\n",
       "      <td>0.855656</td>\n",
       "      <td>0.743370</td>\n",
       "      <td>0.600935</td>\n",
       "      <td>0.539323</td>\n",
       "      <td>0.535827</td>\n",
       "      <td>0.530099</td>\n",
       "      <td>0.538931</td>\n",
       "      <td>0.688710</td>\n",
       "      <td>0.629678</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.720828</td>\n",
       "      <td>3.447748</td>\n",
       "      <td>0.836235</td>\n",
       "      <td>0.739087</td>\n",
       "      <td>0.597451</td>\n",
       "      <td>0.556955</td>\n",
       "      <td>0.519649</td>\n",
       "      <td>0.549075</td>\n",
       "      <td>0.557233</td>\n",
       "      <td>0.685906</td>\n",
       "      <td>0.629851</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.639618</td>\n",
       "      <td>3.361385</td>\n",
       "      <td>0.862302</td>\n",
       "      <td>0.751008</td>\n",
       "      <td>0.615803</td>\n",
       "      <td>0.567277</td>\n",
       "      <td>0.542689</td>\n",
       "      <td>0.555890</td>\n",
       "      <td>0.558928</td>\n",
       "      <td>0.702759</td>\n",
       "      <td>0.644759</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.639768</td>\n",
       "      <td>3.294987</td>\n",
       "      <td>0.863383</td>\n",
       "      <td>0.771413</td>\n",
       "      <td>0.640870</td>\n",
       "      <td>0.579885</td>\n",
       "      <td>0.555951</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.558895</td>\n",
       "      <td>0.717382</td>\n",
       "      <td>0.657370</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.548143</td>\n",
       "      <td>3.347395</td>\n",
       "      <td>0.854678</td>\n",
       "      <td>0.753966</td>\n",
       "      <td>0.630414</td>\n",
       "      <td>0.571192</td>\n",
       "      <td>0.560109</td>\n",
       "      <td>0.574476</td>\n",
       "      <td>0.586169</td>\n",
       "      <td>0.706017</td>\n",
       "      <td>0.654645</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.478572</td>\n",
       "      <td>3.235585</td>\n",
       "      <td>0.870956</td>\n",
       "      <td>0.781181</td>\n",
       "      <td>0.657343</td>\n",
       "      <td>0.598384</td>\n",
       "      <td>0.567482</td>\n",
       "      <td>0.586891</td>\n",
       "      <td>0.587751</td>\n",
       "      <td>0.730309</td>\n",
       "      <td>0.672467</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.445199</td>\n",
       "      <td>3.193153</td>\n",
       "      <td>0.878065</td>\n",
       "      <td>0.786240</td>\n",
       "      <td>0.664090</td>\n",
       "      <td>0.625455</td>\n",
       "      <td>0.572825</td>\n",
       "      <td>0.613356</td>\n",
       "      <td>0.597133</td>\n",
       "      <td>0.741620</td>\n",
       "      <td>0.684577</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.414681</td>\n",
       "      <td>3.143812</td>\n",
       "      <td>0.881311</td>\n",
       "      <td>0.787808</td>\n",
       "      <td>0.679653</td>\n",
       "      <td>0.624865</td>\n",
       "      <td>0.588373</td>\n",
       "      <td>0.610827</td>\n",
       "      <td>0.618389</td>\n",
       "      <td>0.746521</td>\n",
       "      <td>0.691895</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.376518</td>\n",
       "      <td>3.133322</td>\n",
       "      <td>0.881156</td>\n",
       "      <td>0.787334</td>\n",
       "      <td>0.685356</td>\n",
       "      <td>0.631016</td>\n",
       "      <td>0.581217</td>\n",
       "      <td>0.608649</td>\n",
       "      <td>0.591848</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.689376</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.341904</td>\n",
       "      <td>3.112283</td>\n",
       "      <td>0.878632</td>\n",
       "      <td>0.790890</td>\n",
       "      <td>0.688991</td>\n",
       "      <td>0.635741</td>\n",
       "      <td>0.590941</td>\n",
       "      <td>0.627285</td>\n",
       "      <td>0.618803</td>\n",
       "      <td>0.751529</td>\n",
       "      <td>0.697476</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.311324</td>\n",
       "      <td>3.086094</td>\n",
       "      <td>0.882856</td>\n",
       "      <td>0.794961</td>\n",
       "      <td>0.688992</td>\n",
       "      <td>0.633686</td>\n",
       "      <td>0.601249</td>\n",
       "      <td>0.642972</td>\n",
       "      <td>0.631757</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.703420</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.274921</td>\n",
       "      <td>3.041742</td>\n",
       "      <td>0.884092</td>\n",
       "      <td>0.810734</td>\n",
       "      <td>0.707889</td>\n",
       "      <td>0.657357</td>\n",
       "      <td>0.611839</td>\n",
       "      <td>0.645368</td>\n",
       "      <td>0.613518</td>\n",
       "      <td>0.767808</td>\n",
       "      <td>0.712368</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.235580</td>\n",
       "      <td>3.032179</td>\n",
       "      <td>0.888162</td>\n",
       "      <td>0.812713</td>\n",
       "      <td>0.705351</td>\n",
       "      <td>0.666488</td>\n",
       "      <td>0.604204</td>\n",
       "      <td>0.652989</td>\n",
       "      <td>0.636227</td>\n",
       "      <td>0.770960</td>\n",
       "      <td>0.716566</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.235768</td>\n",
       "      <td>3.061273</td>\n",
       "      <td>0.885947</td>\n",
       "      <td>0.805301</td>\n",
       "      <td>0.702795</td>\n",
       "      <td>0.657384</td>\n",
       "      <td>0.605053</td>\n",
       "      <td>0.634004</td>\n",
       "      <td>0.639254</td>\n",
       "      <td>0.765671</td>\n",
       "      <td>0.711363</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.181117</td>\n",
       "      <td>2.998633</td>\n",
       "      <td>0.892798</td>\n",
       "      <td>0.815847</td>\n",
       "      <td>0.723232</td>\n",
       "      <td>0.676634</td>\n",
       "      <td>0.625411</td>\n",
       "      <td>0.656862</td>\n",
       "      <td>0.657796</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.727967</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.164099</td>\n",
       "      <td>2.980504</td>\n",
       "      <td>0.890789</td>\n",
       "      <td>0.820866</td>\n",
       "      <td>0.724156</td>\n",
       "      <td>0.672905</td>\n",
       "      <td>0.620720</td>\n",
       "      <td>0.659405</td>\n",
       "      <td>0.653256</td>\n",
       "      <td>0.779854</td>\n",
       "      <td>0.727160</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.121311</td>\n",
       "      <td>2.982890</td>\n",
       "      <td>0.895992</td>\n",
       "      <td>0.813341</td>\n",
       "      <td>0.721665</td>\n",
       "      <td>0.673636</td>\n",
       "      <td>0.622107</td>\n",
       "      <td>0.664520</td>\n",
       "      <td>0.660569</td>\n",
       "      <td>0.778839</td>\n",
       "      <td>0.728165</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.080203</td>\n",
       "      <td>2.920146</td>\n",
       "      <td>0.895632</td>\n",
       "      <td>0.830622</td>\n",
       "      <td>0.740988</td>\n",
       "      <td>0.689068</td>\n",
       "      <td>0.637581</td>\n",
       "      <td>0.675788</td>\n",
       "      <td>0.672277</td>\n",
       "      <td>0.791619</td>\n",
       "      <td>0.741048</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.034468</td>\n",
       "      <td>2.919588</td>\n",
       "      <td>0.899495</td>\n",
       "      <td>0.834591</td>\n",
       "      <td>0.738866</td>\n",
       "      <td>0.694945</td>\n",
       "      <td>0.639988</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.676104</td>\n",
       "      <td>0.794503</td>\n",
       "      <td>0.745419</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.006769</td>\n",
       "      <td>2.901765</td>\n",
       "      <td>0.898980</td>\n",
       "      <td>0.833127</td>\n",
       "      <td>0.747303</td>\n",
       "      <td>0.705676</td>\n",
       "      <td>0.639005</td>\n",
       "      <td>0.691520</td>\n",
       "      <td>0.678340</td>\n",
       "      <td>0.798643</td>\n",
       "      <td>0.748218</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.961825</td>\n",
       "      <td>2.880265</td>\n",
       "      <td>0.898516</td>\n",
       "      <td>0.839086</td>\n",
       "      <td>0.748609</td>\n",
       "      <td>0.708925</td>\n",
       "      <td>0.644140</td>\n",
       "      <td>0.694199</td>\n",
       "      <td>0.693789</td>\n",
       "      <td>0.801140</td>\n",
       "      <td>0.752523</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.938110</td>\n",
       "      <td>2.872625</td>\n",
       "      <td>0.902741</td>\n",
       "      <td>0.841063</td>\n",
       "      <td>0.754895</td>\n",
       "      <td>0.710416</td>\n",
       "      <td>0.648071</td>\n",
       "      <td>0.701359</td>\n",
       "      <td>0.696816</td>\n",
       "      <td>0.804653</td>\n",
       "      <td>0.756507</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.881670</td>\n",
       "      <td>2.847809</td>\n",
       "      <td>0.909747</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.762547</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.650463</td>\n",
       "      <td>0.707655</td>\n",
       "      <td>0.701484</td>\n",
       "      <td>0.811891</td>\n",
       "      <td>0.762624</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.861300</td>\n",
       "      <td>2.857677</td>\n",
       "      <td>0.905110</td>\n",
       "      <td>0.841903</td>\n",
       "      <td>0.757738</td>\n",
       "      <td>0.720039</td>\n",
       "      <td>0.652850</td>\n",
       "      <td>0.711295</td>\n",
       "      <td>0.698647</td>\n",
       "      <td>0.808485</td>\n",
       "      <td>0.761035</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.850459</td>\n",
       "      <td>2.844098</td>\n",
       "      <td>0.906450</td>\n",
       "      <td>0.847332</td>\n",
       "      <td>0.761482</td>\n",
       "      <td>0.721679</td>\n",
       "      <td>0.655924</td>\n",
       "      <td>0.711862</td>\n",
       "      <td>0.707175</td>\n",
       "      <td>0.811503</td>\n",
       "      <td>0.764344</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.788511</td>\n",
       "      <td>2.835534</td>\n",
       "      <td>0.906707</td>\n",
       "      <td>0.848275</td>\n",
       "      <td>0.766139</td>\n",
       "      <td>0.725145</td>\n",
       "      <td>0.659079</td>\n",
       "      <td>0.712190</td>\n",
       "      <td>0.710644</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.766657</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.776653</td>\n",
       "      <td>2.820805</td>\n",
       "      <td>0.907943</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.771274</td>\n",
       "      <td>0.728177</td>\n",
       "      <td>0.660884</td>\n",
       "      <td>0.713172</td>\n",
       "      <td>0.711644</td>\n",
       "      <td>0.816885</td>\n",
       "      <td>0.769053</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.734579</td>\n",
       "      <td>2.823560</td>\n",
       "      <td>0.907222</td>\n",
       "      <td>0.851406</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>0.730564</td>\n",
       "      <td>0.659790</td>\n",
       "      <td>0.718015</td>\n",
       "      <td>0.714433</td>\n",
       "      <td>0.817353</td>\n",
       "      <td>0.770082</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.738128</td>\n",
       "      <td>2.820041</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.851093</td>\n",
       "      <td>0.771296</td>\n",
       "      <td>0.729124</td>\n",
       "      <td>0.661376</td>\n",
       "      <td>0.717377</td>\n",
       "      <td>0.717206</td>\n",
       "      <td>0.816925</td>\n",
       "      <td>0.770271</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.732873</td>\n",
       "      <td>2.822131</td>\n",
       "      <td>0.906810</td>\n",
       "      <td>0.851303</td>\n",
       "      <td>0.773831</td>\n",
       "      <td>0.731852</td>\n",
       "      <td>0.662194</td>\n",
       "      <td>0.719994</td>\n",
       "      <td>0.718434</td>\n",
       "      <td>0.818100</td>\n",
       "      <td>0.771580</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.743511</td>\n",
       "      <td>2.825693</td>\n",
       "      <td>0.905831</td>\n",
       "      <td>0.852348</td>\n",
       "      <td>0.774475</td>\n",
       "      <td>0.731246</td>\n",
       "      <td>0.664156</td>\n",
       "      <td>0.720968</td>\n",
       "      <td>0.718742</td>\n",
       "      <td>0.818127</td>\n",
       "      <td>0.772049</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.719936</td>\n",
       "      <td>2.823264</td>\n",
       "      <td>0.907944</td>\n",
       "      <td>0.852034</td>\n",
       "      <td>0.774109</td>\n",
       "      <td>0.731913</td>\n",
       "      <td>0.664592</td>\n",
       "      <td>0.721428</td>\n",
       "      <td>0.720282</td>\n",
       "      <td>0.818661</td>\n",
       "      <td>0.772667</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.698428</td>\n",
       "      <td>2.821565</td>\n",
       "      <td>0.907068</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>0.774214</td>\n",
       "      <td>0.731930</td>\n",
       "      <td>0.664375</td>\n",
       "      <td>0.721098</td>\n",
       "      <td>0.719058</td>\n",
       "      <td>0.818528</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instructor = cs.SingleInstruction()\n",
    "c_out = 16\n",
    "learner = cs.cs_learner(get_data(size=128, stats=lip_stats), models.resnet18, instructor, td_c=c_out, pretrained=False,\n",
    "                        loss_func=loss, callback_fns=pose.Pckh)\n",
    "learner.fit_one_cycle(40, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data = get_data(size=256, stats=lip_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.939197</td>\n",
       "      <td>4.445555</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.828330</td>\n",
       "      <td>0.720754</td>\n",
       "      <td>0.672919</td>\n",
       "      <td>0.542080</td>\n",
       "      <td>0.521365</td>\n",
       "      <td>0.489230</td>\n",
       "      <td>0.781056</td>\n",
       "      <td>0.681136</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.698236</td>\n",
       "      <td>4.245080</td>\n",
       "      <td>0.900268</td>\n",
       "      <td>0.837623</td>\n",
       "      <td>0.744419</td>\n",
       "      <td>0.705405</td>\n",
       "      <td>0.572647</td>\n",
       "      <td>0.577018</td>\n",
       "      <td>0.541146</td>\n",
       "      <td>0.799338</td>\n",
       "      <td>0.709569</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.568211</td>\n",
       "      <td>4.136446</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.845249</td>\n",
       "      <td>0.758738</td>\n",
       "      <td>0.723368</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.617344</td>\n",
       "      <td>0.578236</td>\n",
       "      <td>0.810862</td>\n",
       "      <td>0.729218</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.458267</td>\n",
       "      <td>4.058024</td>\n",
       "      <td>0.910313</td>\n",
       "      <td>0.850258</td>\n",
       "      <td>0.768633</td>\n",
       "      <td>0.738419</td>\n",
       "      <td>0.612968</td>\n",
       "      <td>0.650701</td>\n",
       "      <td>0.616630</td>\n",
       "      <td>0.819035</td>\n",
       "      <td>0.745131</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.384262</td>\n",
       "      <td>3.996515</td>\n",
       "      <td>0.915362</td>\n",
       "      <td>0.857409</td>\n",
       "      <td>0.779325</td>\n",
       "      <td>0.746540</td>\n",
       "      <td>0.627485</td>\n",
       "      <td>0.675263</td>\n",
       "      <td>0.649333</td>\n",
       "      <td>0.826741</td>\n",
       "      <td>0.758656</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.326247</td>\n",
       "      <td>3.968316</td>\n",
       "      <td>0.915568</td>\n",
       "      <td>0.859656</td>\n",
       "      <td>0.782606</td>\n",
       "      <td>0.753451</td>\n",
       "      <td>0.639219</td>\n",
       "      <td>0.687484</td>\n",
       "      <td>0.670029</td>\n",
       "      <td>0.829826</td>\n",
       "      <td>0.766081</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.270391</td>\n",
       "      <td>3.938139</td>\n",
       "      <td>0.919431</td>\n",
       "      <td>0.864980</td>\n",
       "      <td>0.787340</td>\n",
       "      <td>0.761926</td>\n",
       "      <td>0.648984</td>\n",
       "      <td>0.701109</td>\n",
       "      <td>0.685660</td>\n",
       "      <td>0.835381</td>\n",
       "      <td>0.774362</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.250793</td>\n",
       "      <td>3.907947</td>\n",
       "      <td>0.919792</td>\n",
       "      <td>0.869838</td>\n",
       "      <td>0.795836</td>\n",
       "      <td>0.763763</td>\n",
       "      <td>0.654991</td>\n",
       "      <td>0.708650</td>\n",
       "      <td>0.699672</td>\n",
       "      <td>0.839240</td>\n",
       "      <td>0.780092</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.210537</td>\n",
       "      <td>3.892767</td>\n",
       "      <td>0.920513</td>\n",
       "      <td>0.868531</td>\n",
       "      <td>0.794734</td>\n",
       "      <td>0.767617</td>\n",
       "      <td>0.657878</td>\n",
       "      <td>0.717691</td>\n",
       "      <td>0.706960</td>\n",
       "      <td>0.839748</td>\n",
       "      <td>0.782759</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.184113</td>\n",
       "      <td>3.887785</td>\n",
       "      <td>0.920256</td>\n",
       "      <td>0.868743</td>\n",
       "      <td>0.799884</td>\n",
       "      <td>0.767479</td>\n",
       "      <td>0.656628</td>\n",
       "      <td>0.717378</td>\n",
       "      <td>0.713971</td>\n",
       "      <td>0.840963</td>\n",
       "      <td>0.784027</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.168561</td>\n",
       "      <td>3.868415</td>\n",
       "      <td>0.922265</td>\n",
       "      <td>0.873335</td>\n",
       "      <td>0.800678</td>\n",
       "      <td>0.769187</td>\n",
       "      <td>0.663062</td>\n",
       "      <td>0.724498</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.843260</td>\n",
       "      <td>0.787476</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.124402</td>\n",
       "      <td>3.863730</td>\n",
       "      <td>0.923501</td>\n",
       "      <td>0.873233</td>\n",
       "      <td>0.801604</td>\n",
       "      <td>0.770961</td>\n",
       "      <td>0.661806</td>\n",
       "      <td>0.729872</td>\n",
       "      <td>0.726673</td>\n",
       "      <td>0.844208</td>\n",
       "      <td>0.789740</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.078165</td>\n",
       "      <td>3.852232</td>\n",
       "      <td>0.923759</td>\n",
       "      <td>0.876575</td>\n",
       "      <td>0.806290</td>\n",
       "      <td>0.773001</td>\n",
       "      <td>0.670156</td>\n",
       "      <td>0.734101</td>\n",
       "      <td>0.725580</td>\n",
       "      <td>0.846772</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.067941</td>\n",
       "      <td>3.848069</td>\n",
       "      <td>0.925922</td>\n",
       "      <td>0.875006</td>\n",
       "      <td>0.806175</td>\n",
       "      <td>0.776602</td>\n",
       "      <td>0.670921</td>\n",
       "      <td>0.737921</td>\n",
       "      <td>0.730671</td>\n",
       "      <td>0.847760</td>\n",
       "      <td>0.794745</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.042357</td>\n",
       "      <td>3.838513</td>\n",
       "      <td>0.923089</td>\n",
       "      <td>0.876936</td>\n",
       "      <td>0.808921</td>\n",
       "      <td>0.775526</td>\n",
       "      <td>0.674622</td>\n",
       "      <td>0.740023</td>\n",
       "      <td>0.732418</td>\n",
       "      <td>0.847934</td>\n",
       "      <td>0.795864</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.015909</td>\n",
       "      <td>3.833871</td>\n",
       "      <td>0.925252</td>\n",
       "      <td>0.877302</td>\n",
       "      <td>0.809307</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.674951</td>\n",
       "      <td>0.742303</td>\n",
       "      <td>0.735812</td>\n",
       "      <td>0.849016</td>\n",
       "      <td>0.797231</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.018998</td>\n",
       "      <td>3.826995</td>\n",
       "      <td>0.923089</td>\n",
       "      <td>0.877826</td>\n",
       "      <td>0.807730</td>\n",
       "      <td>0.776670</td>\n",
       "      <td>0.675013</td>\n",
       "      <td>0.745587</td>\n",
       "      <td>0.741055</td>\n",
       "      <td>0.848148</td>\n",
       "      <td>0.797675</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.973557</td>\n",
       "      <td>3.820290</td>\n",
       "      <td>0.924943</td>\n",
       "      <td>0.877620</td>\n",
       "      <td>0.810270</td>\n",
       "      <td>0.780263</td>\n",
       "      <td>0.678334</td>\n",
       "      <td>0.746907</td>\n",
       "      <td>0.744665</td>\n",
       "      <td>0.850057</td>\n",
       "      <td>0.799906</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.970107</td>\n",
       "      <td>3.818985</td>\n",
       "      <td>0.925252</td>\n",
       "      <td>0.877672</td>\n",
       "      <td>0.813085</td>\n",
       "      <td>0.781613</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.745385</td>\n",
       "      <td>0.748373</td>\n",
       "      <td>0.851166</td>\n",
       "      <td>0.800696</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.941996</td>\n",
       "      <td>3.822423</td>\n",
       "      <td>0.925665</td>\n",
       "      <td>0.878666</td>\n",
       "      <td>0.812095</td>\n",
       "      <td>0.780016</td>\n",
       "      <td>0.680353</td>\n",
       "      <td>0.749003</td>\n",
       "      <td>0.740678</td>\n",
       "      <td>0.850899</td>\n",
       "      <td>0.800565</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.942858</td>\n",
       "      <td>3.820156</td>\n",
       "      <td>0.925768</td>\n",
       "      <td>0.880072</td>\n",
       "      <td>0.813401</td>\n",
       "      <td>0.780617</td>\n",
       "      <td>0.678123</td>\n",
       "      <td>0.749660</td>\n",
       "      <td>0.748352</td>\n",
       "      <td>0.851753</td>\n",
       "      <td>0.801660</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.920711</td>\n",
       "      <td>3.817956</td>\n",
       "      <td>0.925922</td>\n",
       "      <td>0.881747</td>\n",
       "      <td>0.814654</td>\n",
       "      <td>0.784492</td>\n",
       "      <td>0.681657</td>\n",
       "      <td>0.747441</td>\n",
       "      <td>0.750615</td>\n",
       "      <td>0.853449</td>\n",
       "      <td>0.803199</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.917474</td>\n",
       "      <td>3.817139</td>\n",
       "      <td>0.926128</td>\n",
       "      <td>0.880960</td>\n",
       "      <td>0.812845</td>\n",
       "      <td>0.782315</td>\n",
       "      <td>0.681716</td>\n",
       "      <td>0.753649</td>\n",
       "      <td>0.749383</td>\n",
       "      <td>0.852341</td>\n",
       "      <td>0.803174</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.885610</td>\n",
       "      <td>3.811939</td>\n",
       "      <td>0.927468</td>\n",
       "      <td>0.880960</td>\n",
       "      <td>0.817430</td>\n",
       "      <td>0.785205</td>\n",
       "      <td>0.684284</td>\n",
       "      <td>0.756003</td>\n",
       "      <td>0.751219</td>\n",
       "      <td>0.854504</td>\n",
       "      <td>0.805389</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.891574</td>\n",
       "      <td>3.808800</td>\n",
       "      <td>0.924892</td>\n",
       "      <td>0.880802</td>\n",
       "      <td>0.814094</td>\n",
       "      <td>0.784593</td>\n",
       "      <td>0.683843</td>\n",
       "      <td>0.752131</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.852835</td>\n",
       "      <td>0.803841</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.862295</td>\n",
       "      <td>3.812829</td>\n",
       "      <td>0.927880</td>\n",
       "      <td>0.882683</td>\n",
       "      <td>0.815753</td>\n",
       "      <td>0.785557</td>\n",
       "      <td>0.684386</td>\n",
       "      <td>0.753184</td>\n",
       "      <td>0.748990</td>\n",
       "      <td>0.854718</td>\n",
       "      <td>0.804944</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.842784</td>\n",
       "      <td>3.799230</td>\n",
       "      <td>0.927158</td>\n",
       "      <td>0.883729</td>\n",
       "      <td>0.815633</td>\n",
       "      <td>0.787671</td>\n",
       "      <td>0.687175</td>\n",
       "      <td>0.759095</td>\n",
       "      <td>0.752224</td>\n",
       "      <td>0.855292</td>\n",
       "      <td>0.806805</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.851706</td>\n",
       "      <td>3.813744</td>\n",
       "      <td>0.927983</td>\n",
       "      <td>0.882786</td>\n",
       "      <td>0.816245</td>\n",
       "      <td>0.787650</td>\n",
       "      <td>0.683302</td>\n",
       "      <td>0.757655</td>\n",
       "      <td>0.752304</td>\n",
       "      <td>0.855399</td>\n",
       "      <td>0.806113</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.843175</td>\n",
       "      <td>3.815010</td>\n",
       "      <td>0.928395</td>\n",
       "      <td>0.884197</td>\n",
       "      <td>0.816333</td>\n",
       "      <td>0.788204</td>\n",
       "      <td>0.687061</td>\n",
       "      <td>0.758633</td>\n",
       "      <td>0.754288</td>\n",
       "      <td>0.856027</td>\n",
       "      <td>0.807405</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.797127</td>\n",
       "      <td>3.812813</td>\n",
       "      <td>0.927416</td>\n",
       "      <td>0.884722</td>\n",
       "      <td>0.816903</td>\n",
       "      <td>0.789487</td>\n",
       "      <td>0.685107</td>\n",
       "      <td>0.758043</td>\n",
       "      <td>0.749672</td>\n",
       "      <td>0.856347</td>\n",
       "      <td>0.806739</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.823823</td>\n",
       "      <td>3.809088</td>\n",
       "      <td>0.927210</td>\n",
       "      <td>0.885554</td>\n",
       "      <td>0.819349</td>\n",
       "      <td>0.790110</td>\n",
       "      <td>0.685537</td>\n",
       "      <td>0.757650</td>\n",
       "      <td>0.753213</td>\n",
       "      <td>0.857255</td>\n",
       "      <td>0.807694</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.819845</td>\n",
       "      <td>3.817388</td>\n",
       "      <td>0.927158</td>\n",
       "      <td>0.884981</td>\n",
       "      <td>0.816911</td>\n",
       "      <td>0.788653</td>\n",
       "      <td>0.684065</td>\n",
       "      <td>0.758111</td>\n",
       "      <td>0.753371</td>\n",
       "      <td>0.856147</td>\n",
       "      <td>0.806862</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.790078</td>\n",
       "      <td>3.816190</td>\n",
       "      <td>0.926849</td>\n",
       "      <td>0.884612</td>\n",
       "      <td>0.816475</td>\n",
       "      <td>0.788497</td>\n",
       "      <td>0.685317</td>\n",
       "      <td>0.758963</td>\n",
       "      <td>0.754363</td>\n",
       "      <td>0.855826</td>\n",
       "      <td>0.807068</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.799158</td>\n",
       "      <td>3.818783</td>\n",
       "      <td>0.927261</td>\n",
       "      <td>0.885190</td>\n",
       "      <td>0.817728</td>\n",
       "      <td>0.789214</td>\n",
       "      <td>0.685322</td>\n",
       "      <td>0.758627</td>\n",
       "      <td>0.752601</td>\n",
       "      <td>0.856561</td>\n",
       "      <td>0.807290</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.802913</td>\n",
       "      <td>3.817184</td>\n",
       "      <td>0.927571</td>\n",
       "      <td>0.884981</td>\n",
       "      <td>0.820328</td>\n",
       "      <td>0.789052</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.760005</td>\n",
       "      <td>0.755746</td>\n",
       "      <td>0.857189</td>\n",
       "      <td>0.808467</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.795915</td>\n",
       "      <td>3.819767</td>\n",
       "      <td>0.926798</td>\n",
       "      <td>0.885607</td>\n",
       "      <td>0.818813</td>\n",
       "      <td>0.789046</td>\n",
       "      <td>0.687120</td>\n",
       "      <td>0.760004</td>\n",
       "      <td>0.754590</td>\n",
       "      <td>0.856775</td>\n",
       "      <td>0.808080</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.786920</td>\n",
       "      <td>3.820057</td>\n",
       "      <td>0.927571</td>\n",
       "      <td>0.884197</td>\n",
       "      <td>0.820171</td>\n",
       "      <td>0.789222</td>\n",
       "      <td>0.688373</td>\n",
       "      <td>0.759538</td>\n",
       "      <td>0.754829</td>\n",
       "      <td>0.856988</td>\n",
       "      <td>0.808369</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.763070</td>\n",
       "      <td>3.823327</td>\n",
       "      <td>0.927313</td>\n",
       "      <td>0.885452</td>\n",
       "      <td>0.818862</td>\n",
       "      <td>0.789881</td>\n",
       "      <td>0.686797</td>\n",
       "      <td>0.759279</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.857082</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.785536</td>\n",
       "      <td>3.817467</td>\n",
       "      <td>0.927158</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>0.819077</td>\n",
       "      <td>0.791161</td>\n",
       "      <td>0.685865</td>\n",
       "      <td>0.760724</td>\n",
       "      <td>0.756055</td>\n",
       "      <td>0.857442</td>\n",
       "      <td>0.808550</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.773131</td>\n",
       "      <td>3.819180</td>\n",
       "      <td>0.926901</td>\n",
       "      <td>0.884982</td>\n",
       "      <td>0.819673</td>\n",
       "      <td>0.789824</td>\n",
       "      <td>0.685864</td>\n",
       "      <td>0.759148</td>\n",
       "      <td>0.754827</td>\n",
       "      <td>0.857042</td>\n",
       "      <td>0.807973</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(40, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor = cs.SingleInstruction()\n",
    "c_out = 16\n",
    "learner = cs.cs_learner(data, models.resnet18, instructor, td_c=c_out, pretrained=False,\n",
    "                        loss_func=loss, callback_fns=pose.Pckh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='466', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      21.24% [99/466 00:18<01:09 7.4619]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.32E-06\n",
      "Min loss divided by 10: 1.10E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('baselinev2-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (29866 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (8/16) (128, 128),Pose (7/16) (128, 128),Pose (6/16) (128, 128),Pose (14/16) (128, 128),Pose (1/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (14/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Test: None, model=CounterStream(\n",
       "  (bu_body): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (td): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TDHead(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (laterals): ModuleList(\n",
       "    (0): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (12): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (13): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (14): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (15): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (emb): Embedding(1, 512)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function loss at 0x7f45536b7ea0>, metrics=[], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/labs/waic/omrik/LIP'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'pose.Pckh'>], callbacks=[SingleInstruction], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Lambda()\n",
       "  (17): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (28): ReLU(inplace=True)\n",
       "  (29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (30): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (33): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (34): ReLU(inplace=True)\n",
       "  (35): Lambda()\n",
       "  (36): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (39): ReLU(inplace=True)\n",
       "  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (42): ReLU(inplace=True)\n",
       "  (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (45): ReLU(inplace=True)\n",
       "  (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (49): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Lambda()\n",
       "  (55): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (56): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (58): ReLU(inplace=True)\n",
       "  (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (61): ReLU(inplace=True)\n",
       "  (62): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (64): ReLU(inplace=True)\n",
       "  (65): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (66): ReLU(inplace=True)\n",
       "  (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (69): ReLU(inplace=True)\n",
       "  (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (72): ReLU(inplace=True)\n",
       "  (73): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (76): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (77): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (78): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (79): ReLU(inplace=True)\n",
       "  (80): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (81): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (82): ReLU(inplace=True)\n",
       "  (83): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (84): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (85): ReLU(inplace=True)\n",
       "  (86): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (87): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (88): ReLU(inplace=True)\n",
       "  (89): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (90): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (91): ReLU(inplace=True)\n",
       "  (92): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (93): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (94): ReLU(inplace=True)\n",
       "  (95): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (96): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (97): ReLU(inplace=True)\n",
       "  (98): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (99): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (100): ReLU(inplace=True)\n",
       "  (101): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (102): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (103): ReLU(inplace=True)\n",
       "  (104): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (105): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (106): ReLU(inplace=True)\n",
       "  (107): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (108): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (109): ReLU(inplace=True)\n",
       "  (110): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (111): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (112): ReLU(inplace=True)\n",
       "  (113): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (114): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (115): ReLU(inplace=True)\n",
       "  (116): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (117): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (118): ReLU(inplace=True)\n",
       "  (119): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (120): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (121): ReLU(inplace=True)\n",
       "  (122): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (123): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (124): ReLU(inplace=True)\n",
       "  (125): Embedding(1, 512)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('baselinev2-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data = get_data(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('baselinev2-256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor = cs.SingleInstruction()\n",
    "c_out = 16\n",
    "learner = cs.cs_learner(get_data(128), models.resnet18, instructor, td_c=c_out, pretrained=True,\n",
    "                        loss_func=loss, callback_fns=pose.Pckh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='98' class='' max='466', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      21.03% [98/466 00:12<00:46 10.0052]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.91E-02\n",
      "Min loss divided by 10: 2.29E-01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.097866</td>\n",
       "      <td>3.841648</td>\n",
       "      <td>0.797084</td>\n",
       "      <td>0.620594</td>\n",
       "      <td>0.464423</td>\n",
       "      <td>0.431735</td>\n",
       "      <td>0.453119</td>\n",
       "      <td>0.446390</td>\n",
       "      <td>0.473141</td>\n",
       "      <td>0.582797</td>\n",
       "      <td>0.534352</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.976435</td>\n",
       "      <td>3.687898</td>\n",
       "      <td>0.813002</td>\n",
       "      <td>0.664535</td>\n",
       "      <td>0.528684</td>\n",
       "      <td>0.484477</td>\n",
       "      <td>0.464774</td>\n",
       "      <td>0.459365</td>\n",
       "      <td>0.492349</td>\n",
       "      <td>0.626626</td>\n",
       "      <td>0.566810</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.874358</td>\n",
       "      <td>3.745656</td>\n",
       "      <td>0.830105</td>\n",
       "      <td>0.681113</td>\n",
       "      <td>0.503738</td>\n",
       "      <td>0.494618</td>\n",
       "      <td>0.452172</td>\n",
       "      <td>0.474756</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>0.631607</td>\n",
       "      <td>0.568901</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.727231</td>\n",
       "      <td>3.466380</td>\n",
       "      <td>0.854162</td>\n",
       "      <td>0.730122</td>\n",
       "      <td>0.618539</td>\n",
       "      <td>0.556644</td>\n",
       "      <td>0.517199</td>\n",
       "      <td>0.506712</td>\n",
       "      <td>0.523376</td>\n",
       "      <td>0.693358</td>\n",
       "      <td>0.625134</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.628429</td>\n",
       "      <td>3.877989</td>\n",
       "      <td>0.863332</td>\n",
       "      <td>0.748129</td>\n",
       "      <td>0.613463</td>\n",
       "      <td>0.538797</td>\n",
       "      <td>0.451068</td>\n",
       "      <td>0.198781</td>\n",
       "      <td>0.257686</td>\n",
       "      <td>0.694760</td>\n",
       "      <td>0.548857</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.521747</td>\n",
       "      <td>3.225345</td>\n",
       "      <td>0.874665</td>\n",
       "      <td>0.776060</td>\n",
       "      <td>0.655458</td>\n",
       "      <td>0.606853</td>\n",
       "      <td>0.561142</td>\n",
       "      <td>0.594724</td>\n",
       "      <td>0.559874</td>\n",
       "      <td>0.731538</td>\n",
       "      <td>0.670261</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.405599</td>\n",
       "      <td>3.126620</td>\n",
       "      <td>0.880177</td>\n",
       "      <td>0.794232</td>\n",
       "      <td>0.691099</td>\n",
       "      <td>0.640095</td>\n",
       "      <td>0.589676</td>\n",
       "      <td>0.609084</td>\n",
       "      <td>0.589172</td>\n",
       "      <td>0.754347</td>\n",
       "      <td>0.693566</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.321985</td>\n",
       "      <td>3.052960</td>\n",
       "      <td>0.888471</td>\n",
       "      <td>0.803206</td>\n",
       "      <td>0.700659</td>\n",
       "      <td>0.647937</td>\n",
       "      <td>0.603890</td>\n",
       "      <td>0.639309</td>\n",
       "      <td>0.615075</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.707618</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.225178</td>\n",
       "      <td>3.018742</td>\n",
       "      <td>0.889295</td>\n",
       "      <td>0.811200</td>\n",
       "      <td>0.711056</td>\n",
       "      <td>0.664343</td>\n",
       "      <td>0.614522</td>\n",
       "      <td>0.652339</td>\n",
       "      <td>0.624862</td>\n",
       "      <td>0.771734</td>\n",
       "      <td>0.717290</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.196515</td>\n",
       "      <td>3.008733</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.813812</td>\n",
       "      <td>0.710699</td>\n",
       "      <td>0.666244</td>\n",
       "      <td>0.615229</td>\n",
       "      <td>0.653198</td>\n",
       "      <td>0.630238</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>0.719044</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 3e-2\n",
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('baselinev2-128-stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (29866 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (8/16) (128, 128),Pose (7/16) (128, 128),Pose (6/16) (128, 128),Pose (14/16) (128, 128),Pose (1/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (14/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Test: None, model=CounterStream(\n",
       "  (bu_body): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (td): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TDHead(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (laterals): ModuleList(\n",
       "    (0): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (12): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (13): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (14): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (15): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (emb): Embedding(1, 512)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function loss at 0x7fef34d4b048>, metrics=[], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/labs/waic/omrik/LIP'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'pose.Pckh'>], callbacks=[SingleInstruction], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Lambda()\n",
       "  (17): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (28): ReLU(inplace=True)\n",
       "  (29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (30): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (33): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (34): ReLU(inplace=True)\n",
       "  (35): Lambda()\n",
       "  (36): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (39): ReLU(inplace=True)\n",
       "  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (42): ReLU(inplace=True)\n",
       "  (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (45): ReLU(inplace=True)\n",
       "  (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (49): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Lambda()\n",
       "  (55): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (56): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (58): ReLU(inplace=True)\n",
       "  (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (61): ReLU(inplace=True)\n",
       "  (62): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (64): ReLU(inplace=True)\n",
       "  (65): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (66): ReLU(inplace=True)\n",
       "  (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (69): ReLU(inplace=True)\n",
       "  (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (72): ReLU(inplace=True)\n",
       "  (73): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (76): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (77): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (78): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (79): ReLU(inplace=True)\n",
       "  (80): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (81): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (82): ReLU(inplace=True)\n",
       "  (83): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (84): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (85): ReLU(inplace=True)\n",
       "  (86): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (87): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (88): ReLU(inplace=True)\n",
       "  (89): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (90): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (91): ReLU(inplace=True)\n",
       "  (92): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (93): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (94): ReLU(inplace=True)\n",
       "  (95): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (96): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (97): ReLU(inplace=True)\n",
       "  (98): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (99): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (100): ReLU(inplace=True)\n",
       "  (101): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (102): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (103): ReLU(inplace=True)\n",
       "  (104): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (105): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (106): ReLU(inplace=True)\n",
       "  (107): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (108): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (109): ReLU(inplace=True)\n",
       "  (110): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (111): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (112): ReLU(inplace=True)\n",
       "  (113): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (114): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (115): ReLU(inplace=True)\n",
       "  (116): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (117): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (118): ReLU(inplace=True)\n",
       "  (119): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (120): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (121): ReLU(inplace=True)\n",
       "  (122): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (123): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (124): ReLU(inplace=True)\n",
       "  (125): Embedding(1, 512)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('baselinev2-128-stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='96' class='' max='466', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.60% [96/466 00:11<00:45 11.5434]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 7.59E-07\n",
      "Min loss divided by 10: 1.20E-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.207350</td>\n",
       "      <td>3.007453</td>\n",
       "      <td>0.891613</td>\n",
       "      <td>0.812924</td>\n",
       "      <td>0.713031</td>\n",
       "      <td>0.668771</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.658226</td>\n",
       "      <td>0.626858</td>\n",
       "      <td>0.774325</td>\n",
       "      <td>0.719974</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.227029</td>\n",
       "      <td>3.011534</td>\n",
       "      <td>0.890068</td>\n",
       "      <td>0.813709</td>\n",
       "      <td>0.711414</td>\n",
       "      <td>0.665825</td>\n",
       "      <td>0.614248</td>\n",
       "      <td>0.655939</td>\n",
       "      <td>0.623543</td>\n",
       "      <td>0.773016</td>\n",
       "      <td>0.718352</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.212544</td>\n",
       "      <td>3.023557</td>\n",
       "      <td>0.890841</td>\n",
       "      <td>0.808074</td>\n",
       "      <td>0.702515</td>\n",
       "      <td>0.660875</td>\n",
       "      <td>0.610799</td>\n",
       "      <td>0.645323</td>\n",
       "      <td>0.614791</td>\n",
       "      <td>0.768396</td>\n",
       "      <td>0.712713</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.220389</td>\n",
       "      <td>3.026373</td>\n",
       "      <td>0.892798</td>\n",
       "      <td>0.811619</td>\n",
       "      <td>0.708444</td>\n",
       "      <td>0.666410</td>\n",
       "      <td>0.611012</td>\n",
       "      <td>0.646705</td>\n",
       "      <td>0.620777</td>\n",
       "      <td>0.772616</td>\n",
       "      <td>0.716163</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.256129</td>\n",
       "      <td>3.044462</td>\n",
       "      <td>0.887029</td>\n",
       "      <td>0.809374</td>\n",
       "      <td>0.698166</td>\n",
       "      <td>0.659180</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.639182</td>\n",
       "      <td>0.609002</td>\n",
       "      <td>0.766286</td>\n",
       "      <td>0.708252</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.186917</td>\n",
       "      <td>3.008080</td>\n",
       "      <td>0.892644</td>\n",
       "      <td>0.816163</td>\n",
       "      <td>0.720048</td>\n",
       "      <td>0.667867</td>\n",
       "      <td>0.609706</td>\n",
       "      <td>0.649849</td>\n",
       "      <td>0.623002</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.719241</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.160174</td>\n",
       "      <td>2.986812</td>\n",
       "      <td>0.891407</td>\n",
       "      <td>0.823157</td>\n",
       "      <td>0.723125</td>\n",
       "      <td>0.670203</td>\n",
       "      <td>0.619971</td>\n",
       "      <td>0.655801</td>\n",
       "      <td>0.629796</td>\n",
       "      <td>0.779733</td>\n",
       "      <td>0.724008</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.146750</td>\n",
       "      <td>2.972959</td>\n",
       "      <td>0.894189</td>\n",
       "      <td>0.821481</td>\n",
       "      <td>0.722436</td>\n",
       "      <td>0.677618</td>\n",
       "      <td>0.625098</td>\n",
       "      <td>0.661557</td>\n",
       "      <td>0.637320</td>\n",
       "      <td>0.781603</td>\n",
       "      <td>0.727465</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.118945</td>\n",
       "      <td>2.950031</td>\n",
       "      <td>0.896301</td>\n",
       "      <td>0.826916</td>\n",
       "      <td>0.724352</td>\n",
       "      <td>0.679168</td>\n",
       "      <td>0.630339</td>\n",
       "      <td>0.673911</td>\n",
       "      <td>0.644185</td>\n",
       "      <td>0.784381</td>\n",
       "      <td>0.732248</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.075075</td>\n",
       "      <td>2.950006</td>\n",
       "      <td>0.893674</td>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.732687</td>\n",
       "      <td>0.685573</td>\n",
       "      <td>0.624325</td>\n",
       "      <td>0.668635</td>\n",
       "      <td>0.637341</td>\n",
       "      <td>0.788267</td>\n",
       "      <td>0.732347</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.055553</td>\n",
       "      <td>2.933806</td>\n",
       "      <td>0.896662</td>\n",
       "      <td>0.833184</td>\n",
       "      <td>0.736236</td>\n",
       "      <td>0.690612</td>\n",
       "      <td>0.627761</td>\n",
       "      <td>0.678166</td>\n",
       "      <td>0.659640</td>\n",
       "      <td>0.791739</td>\n",
       "      <td>0.738586</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.001991</td>\n",
       "      <td>2.919114</td>\n",
       "      <td>0.895322</td>\n",
       "      <td>0.830470</td>\n",
       "      <td>0.739231</td>\n",
       "      <td>0.688339</td>\n",
       "      <td>0.636168</td>\n",
       "      <td>0.678677</td>\n",
       "      <td>0.654328</td>\n",
       "      <td>0.790884</td>\n",
       "      <td>0.738825</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.980719</td>\n",
       "      <td>5.508926</td>\n",
       "      <td>0.897744</td>\n",
       "      <td>0.836215</td>\n",
       "      <td>0.746220</td>\n",
       "      <td>0.696417</td>\n",
       "      <td>0.635137</td>\n",
       "      <td>0.683123</td>\n",
       "      <td>0.661567</td>\n",
       "      <td>0.796613</td>\n",
       "      <td>0.743534</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.918633</td>\n",
       "      <td>2.909985</td>\n",
       "      <td>0.897383</td>\n",
       "      <td>0.836262</td>\n",
       "      <td>0.744551</td>\n",
       "      <td>0.695283</td>\n",
       "      <td>0.637694</td>\n",
       "      <td>0.689803</td>\n",
       "      <td>0.676876</td>\n",
       "      <td>0.795879</td>\n",
       "      <td>0.745946</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.913242</td>\n",
       "      <td>3.154561</td>\n",
       "      <td>0.899650</td>\n",
       "      <td>0.836416</td>\n",
       "      <td>0.749912</td>\n",
       "      <td>0.702936</td>\n",
       "      <td>0.645113</td>\n",
       "      <td>0.697269</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.799631</td>\n",
       "      <td>0.750597</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.872616</td>\n",
       "      <td>6.181575</td>\n",
       "      <td>0.900732</td>\n",
       "      <td>0.837671</td>\n",
       "      <td>0.748276</td>\n",
       "      <td>0.704784</td>\n",
       "      <td>0.646038</td>\n",
       "      <td>0.694412</td>\n",
       "      <td>0.683201</td>\n",
       "      <td>0.800286</td>\n",
       "      <td>0.751173</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.846689</td>\n",
       "      <td>4.454049</td>\n",
       "      <td>0.901865</td>\n",
       "      <td>0.840022</td>\n",
       "      <td>0.753361</td>\n",
       "      <td>0.708150</td>\n",
       "      <td>0.650894</td>\n",
       "      <td>0.697074</td>\n",
       "      <td>0.681506</td>\n",
       "      <td>0.803224</td>\n",
       "      <td>0.753873</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.811521</td>\n",
       "      <td>2.907416</td>\n",
       "      <td>0.899856</td>\n",
       "      <td>0.839914</td>\n",
       "      <td>0.755020</td>\n",
       "      <td>0.707663</td>\n",
       "      <td>0.650026</td>\n",
       "      <td>0.699035</td>\n",
       "      <td>0.679283</td>\n",
       "      <td>0.802983</td>\n",
       "      <td>0.753601</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.813192</td>\n",
       "      <td>7.619845</td>\n",
       "      <td>0.901710</td>\n",
       "      <td>0.839131</td>\n",
       "      <td>0.751706</td>\n",
       "      <td>0.706746</td>\n",
       "      <td>0.647458</td>\n",
       "      <td>0.694781</td>\n",
       "      <td>0.679654</td>\n",
       "      <td>0.802222</td>\n",
       "      <td>0.752251</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.809687</td>\n",
       "      <td>2.897693</td>\n",
       "      <td>0.902947</td>\n",
       "      <td>0.840070</td>\n",
       "      <td>0.754696</td>\n",
       "      <td>0.708001</td>\n",
       "      <td>0.649364</td>\n",
       "      <td>0.698837</td>\n",
       "      <td>0.680656</td>\n",
       "      <td>0.803825</td>\n",
       "      <td>0.754145</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(20, slice(1e-4, lr/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('baselinev2-128-stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('baselinev2-128-stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data = get_data(256)\n",
    "learner.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = \n",
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('baselinev2-256-stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('baselinev2-256-stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(20, slice(, lr/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('baselinev2-256-stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.987631</td>\n",
       "      <td>4.647799</td>\n",
       "      <td>0.663816</td>\n",
       "      <td>0.487432</td>\n",
       "      <td>0.291745</td>\n",
       "      <td>0.267620</td>\n",
       "      <td>0.311118</td>\n",
       "      <td>0.252586</td>\n",
       "      <td>0.362038</td>\n",
       "      <td>0.432427</td>\n",
       "      <td>0.383979</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.532515</td>\n",
       "      <td>4.500963</td>\n",
       "      <td>0.734030</td>\n",
       "      <td>0.545255</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.303495</td>\n",
       "      <td>0.352838</td>\n",
       "      <td>0.341012</td>\n",
       "      <td>0.410783</td>\n",
       "      <td>0.482132</td>\n",
       "      <td>0.437256</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.326157</td>\n",
       "      <td>3.964130</td>\n",
       "      <td>0.796775</td>\n",
       "      <td>0.644439</td>\n",
       "      <td>0.442088</td>\n",
       "      <td>0.393341</td>\n",
       "      <td>0.422508</td>\n",
       "      <td>0.394485</td>\n",
       "      <td>0.443564</td>\n",
       "      <td>0.574063</td>\n",
       "      <td>0.514645</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.140950</td>\n",
       "      <td>3.875656</td>\n",
       "      <td>0.782506</td>\n",
       "      <td>0.668576</td>\n",
       "      <td>0.493354</td>\n",
       "      <td>0.441134</td>\n",
       "      <td>0.436083</td>\n",
       "      <td>0.442654</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.600705</td>\n",
       "      <td>0.541168</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.030505</td>\n",
       "      <td>4.038970</td>\n",
       "      <td>0.782403</td>\n",
       "      <td>0.663064</td>\n",
       "      <td>0.480930</td>\n",
       "      <td>0.452844</td>\n",
       "      <td>0.427617</td>\n",
       "      <td>0.420304</td>\n",
       "      <td>0.458466</td>\n",
       "      <td>0.598996</td>\n",
       "      <td>0.535644</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.891826</td>\n",
       "      <td>3.672553</td>\n",
       "      <td>0.830775</td>\n",
       "      <td>0.686620</td>\n",
       "      <td>0.540898</td>\n",
       "      <td>0.504863</td>\n",
       "      <td>0.475946</td>\n",
       "      <td>0.478994</td>\n",
       "      <td>0.486213</td>\n",
       "      <td>0.644734</td>\n",
       "      <td>0.581480</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.830612</td>\n",
       "      <td>3.655392</td>\n",
       "      <td>0.820626</td>\n",
       "      <td>0.696297</td>\n",
       "      <td>0.548499</td>\n",
       "      <td>0.511501</td>\n",
       "      <td>0.465923</td>\n",
       "      <td>0.490802</td>\n",
       "      <td>0.508279</td>\n",
       "      <td>0.647979</td>\n",
       "      <td>0.585793</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.791111</td>\n",
       "      <td>3.465661</td>\n",
       "      <td>0.849423</td>\n",
       "      <td>0.727908</td>\n",
       "      <td>0.609664</td>\n",
       "      <td>0.549702</td>\n",
       "      <td>0.528624</td>\n",
       "      <td>0.532253</td>\n",
       "      <td>0.534701</td>\n",
       "      <td>0.687749</td>\n",
       "      <td>0.627817</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.707333</td>\n",
       "      <td>3.483002</td>\n",
       "      <td>0.835566</td>\n",
       "      <td>0.728199</td>\n",
       "      <td>0.595639</td>\n",
       "      <td>0.546872</td>\n",
       "      <td>0.527033</td>\n",
       "      <td>0.530554</td>\n",
       "      <td>0.541865</td>\n",
       "      <td>0.680097</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.632650</td>\n",
       "      <td>3.416414</td>\n",
       "      <td>0.838038</td>\n",
       "      <td>0.743633</td>\n",
       "      <td>0.614455</td>\n",
       "      <td>0.558049</td>\n",
       "      <td>0.531377</td>\n",
       "      <td>0.551336</td>\n",
       "      <td>0.530738</td>\n",
       "      <td>0.692035</td>\n",
       "      <td>0.632847</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.628453</td>\n",
       "      <td>3.340008</td>\n",
       "      <td>0.858799</td>\n",
       "      <td>0.747287</td>\n",
       "      <td>0.623968</td>\n",
       "      <td>0.595087</td>\n",
       "      <td>0.535772</td>\n",
       "      <td>0.584247</td>\n",
       "      <td>0.580771</td>\n",
       "      <td>0.709596</td>\n",
       "      <td>0.653830</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.532755</td>\n",
       "      <td>3.248252</td>\n",
       "      <td>0.866217</td>\n",
       "      <td>0.763792</td>\n",
       "      <td>0.655167</td>\n",
       "      <td>0.600435</td>\n",
       "      <td>0.558304</td>\n",
       "      <td>0.589052</td>\n",
       "      <td>0.574810</td>\n",
       "      <td>0.724567</td>\n",
       "      <td>0.666417</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.500346</td>\n",
       "      <td>3.442287</td>\n",
       "      <td>0.848547</td>\n",
       "      <td>0.744371</td>\n",
       "      <td>0.622758</td>\n",
       "      <td>0.584415</td>\n",
       "      <td>0.523663</td>\n",
       "      <td>0.562705</td>\n",
       "      <td>0.570016</td>\n",
       "      <td>0.703253</td>\n",
       "      <td>0.644232</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.462788</td>\n",
       "      <td>3.214971</td>\n",
       "      <td>0.875747</td>\n",
       "      <td>0.761135</td>\n",
       "      <td>0.657538</td>\n",
       "      <td>0.611477</td>\n",
       "      <td>0.567794</td>\n",
       "      <td>0.590127</td>\n",
       "      <td>0.596565</td>\n",
       "      <td>0.729601</td>\n",
       "      <td>0.673422</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.442331</td>\n",
       "      <td>3.174817</td>\n",
       "      <td>0.866423</td>\n",
       "      <td>0.777939</td>\n",
       "      <td>0.675961</td>\n",
       "      <td>0.631593</td>\n",
       "      <td>0.580080</td>\n",
       "      <td>0.608580</td>\n",
       "      <td>0.589714</td>\n",
       "      <td>0.740832</td>\n",
       "      <td>0.683786</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.368907</td>\n",
       "      <td>3.167577</td>\n",
       "      <td>0.873738</td>\n",
       "      <td>0.783843</td>\n",
       "      <td>0.673944</td>\n",
       "      <td>0.628090</td>\n",
       "      <td>0.588596</td>\n",
       "      <td>0.596557</td>\n",
       "      <td>0.601778</td>\n",
       "      <td>0.742889</td>\n",
       "      <td>0.686116</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.371530</td>\n",
       "      <td>3.195189</td>\n",
       "      <td>0.866423</td>\n",
       "      <td>0.786660</td>\n",
       "      <td>0.675813</td>\n",
       "      <td>0.633363</td>\n",
       "      <td>0.578283</td>\n",
       "      <td>0.607411</td>\n",
       "      <td>0.589858</td>\n",
       "      <td>0.743450</td>\n",
       "      <td>0.684996</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.317765</td>\n",
       "      <td>3.068108</td>\n",
       "      <td>0.883217</td>\n",
       "      <td>0.802951</td>\n",
       "      <td>0.700574</td>\n",
       "      <td>0.655087</td>\n",
       "      <td>0.605173</td>\n",
       "      <td>0.631389</td>\n",
       "      <td>0.618650</td>\n",
       "      <td>0.763268</td>\n",
       "      <td>0.707363</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.264729</td>\n",
       "      <td>3.089297</td>\n",
       "      <td>0.880692</td>\n",
       "      <td>0.798036</td>\n",
       "      <td>0.695314</td>\n",
       "      <td>0.645707</td>\n",
       "      <td>0.604398</td>\n",
       "      <td>0.631183</td>\n",
       "      <td>0.614337</td>\n",
       "      <td>0.757806</td>\n",
       "      <td>0.703395</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.239416</td>\n",
       "      <td>3.022310</td>\n",
       "      <td>0.887183</td>\n",
       "      <td>0.815374</td>\n",
       "      <td>0.717443</td>\n",
       "      <td>0.666888</td>\n",
       "      <td>0.622186</td>\n",
       "      <td>0.649110</td>\n",
       "      <td>0.644772</td>\n",
       "      <td>0.774445</td>\n",
       "      <td>0.721851</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.219052</td>\n",
       "      <td>3.035329</td>\n",
       "      <td>0.884762</td>\n",
       "      <td>0.810159</td>\n",
       "      <td>0.705542</td>\n",
       "      <td>0.660832</td>\n",
       "      <td>0.601629</td>\n",
       "      <td>0.650586</td>\n",
       "      <td>0.629108</td>\n",
       "      <td>0.768102</td>\n",
       "      <td>0.713339</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.170683</td>\n",
       "      <td>2.978879</td>\n",
       "      <td>0.893004</td>\n",
       "      <td>0.817356</td>\n",
       "      <td>0.718088</td>\n",
       "      <td>0.674658</td>\n",
       "      <td>0.619309</td>\n",
       "      <td>0.666942</td>\n",
       "      <td>0.658944</td>\n",
       "      <td>0.778478</td>\n",
       "      <td>0.727654</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.164076</td>\n",
       "      <td>3.074272</td>\n",
       "      <td>0.881156</td>\n",
       "      <td>0.800438</td>\n",
       "      <td>0.699253</td>\n",
       "      <td>0.658777</td>\n",
       "      <td>0.593760</td>\n",
       "      <td>0.636111</td>\n",
       "      <td>0.620091</td>\n",
       "      <td>0.762680</td>\n",
       "      <td>0.706029</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.111602</td>\n",
       "      <td>2.948857</td>\n",
       "      <td>0.892798</td>\n",
       "      <td>0.821581</td>\n",
       "      <td>0.734436</td>\n",
       "      <td>0.687138</td>\n",
       "      <td>0.638127</td>\n",
       "      <td>0.675174</td>\n",
       "      <td>0.655373</td>\n",
       "      <td>0.786477</td>\n",
       "      <td>0.736076</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.094481</td>\n",
       "      <td>2.923426</td>\n",
       "      <td>0.900732</td>\n",
       "      <td>0.827387</td>\n",
       "      <td>0.737798</td>\n",
       "      <td>0.691964</td>\n",
       "      <td>0.633798</td>\n",
       "      <td>0.683947</td>\n",
       "      <td>0.666265</td>\n",
       "      <td>0.791993</td>\n",
       "      <td>0.741097</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.043841</td>\n",
       "      <td>2.903039</td>\n",
       "      <td>0.897847</td>\n",
       "      <td>0.827539</td>\n",
       "      <td>0.741780</td>\n",
       "      <td>0.696155</td>\n",
       "      <td>0.639910</td>\n",
       "      <td>0.683890</td>\n",
       "      <td>0.681439</td>\n",
       "      <td>0.793275</td>\n",
       "      <td>0.744423</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.007352</td>\n",
       "      <td>2.954558</td>\n",
       "      <td>0.894601</td>\n",
       "      <td>0.825557</td>\n",
       "      <td>0.728955</td>\n",
       "      <td>0.688104</td>\n",
       "      <td>0.627419</td>\n",
       "      <td>0.663146</td>\n",
       "      <td>0.648028</td>\n",
       "      <td>0.786838</td>\n",
       "      <td>0.732388</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.977718</td>\n",
       "      <td>2.903926</td>\n",
       "      <td>0.897177</td>\n",
       "      <td>0.835111</td>\n",
       "      <td>0.746235</td>\n",
       "      <td>0.702248</td>\n",
       "      <td>0.641569</td>\n",
       "      <td>0.684629</td>\n",
       "      <td>0.679651</td>\n",
       "      <td>0.797588</td>\n",
       "      <td>0.747230</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.912685</td>\n",
       "      <td>2.995401</td>\n",
       "      <td>0.900268</td>\n",
       "      <td>0.840746</td>\n",
       "      <td>0.754775</td>\n",
       "      <td>0.710276</td>\n",
       "      <td>0.646803</td>\n",
       "      <td>0.697258</td>\n",
       "      <td>0.690110</td>\n",
       "      <td>0.803865</td>\n",
       "      <td>0.754598</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.897315</td>\n",
       "      <td>2.847313</td>\n",
       "      <td>0.901041</td>\n",
       "      <td>0.842061</td>\n",
       "      <td>0.763847</td>\n",
       "      <td>0.712842</td>\n",
       "      <td>0.652581</td>\n",
       "      <td>0.704658</td>\n",
       "      <td>0.701042</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>0.759644</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.872580</td>\n",
       "      <td>2.837795</td>\n",
       "      <td>0.907222</td>\n",
       "      <td>0.846182</td>\n",
       "      <td>0.766451</td>\n",
       "      <td>0.717502</td>\n",
       "      <td>0.653342</td>\n",
       "      <td>0.704185</td>\n",
       "      <td>0.701123</td>\n",
       "      <td>0.811637</td>\n",
       "      <td>0.762426</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.839586</td>\n",
       "      <td>2.834182</td>\n",
       "      <td>0.905522</td>\n",
       "      <td>0.846076</td>\n",
       "      <td>0.766517</td>\n",
       "      <td>0.719549</td>\n",
       "      <td>0.655036</td>\n",
       "      <td>0.706875</td>\n",
       "      <td>0.699648</td>\n",
       "      <td>0.811677</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.818932</td>\n",
       "      <td>2.841977</td>\n",
       "      <td>0.903925</td>\n",
       "      <td>0.846653</td>\n",
       "      <td>0.767234</td>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.657650</td>\n",
       "      <td>0.709820</td>\n",
       "      <td>0.699664</td>\n",
       "      <td>0.811877</td>\n",
       "      <td>0.763776</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.783438</td>\n",
       "      <td>3.050344</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>0.771007</td>\n",
       "      <td>0.723231</td>\n",
       "      <td>0.660004</td>\n",
       "      <td>0.715839</td>\n",
       "      <td>0.708183</td>\n",
       "      <td>0.814014</td>\n",
       "      <td>0.767118</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.771501</td>\n",
       "      <td>2.890441</td>\n",
       "      <td>0.905522</td>\n",
       "      <td>0.848845</td>\n",
       "      <td>0.772281</td>\n",
       "      <td>0.725249</td>\n",
       "      <td>0.660498</td>\n",
       "      <td>0.718525</td>\n",
       "      <td>0.714121</td>\n",
       "      <td>0.815163</td>\n",
       "      <td>0.768872</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.728140</td>\n",
       "      <td>2.820809</td>\n",
       "      <td>0.907274</td>\n",
       "      <td>0.850044</td>\n",
       "      <td>0.774058</td>\n",
       "      <td>0.727590</td>\n",
       "      <td>0.659086</td>\n",
       "      <td>0.719443</td>\n",
       "      <td>0.715508</td>\n",
       "      <td>0.816912</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.720135</td>\n",
       "      <td>2.813975</td>\n",
       "      <td>0.906913</td>\n",
       "      <td>0.852343</td>\n",
       "      <td>0.776820</td>\n",
       "      <td>0.730362</td>\n",
       "      <td>0.665240</td>\n",
       "      <td>0.719640</td>\n",
       "      <td>0.715109</td>\n",
       "      <td>0.818755</td>\n",
       "      <td>0.772049</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.716336</td>\n",
       "      <td>2.856610</td>\n",
       "      <td>0.906347</td>\n",
       "      <td>0.852239</td>\n",
       "      <td>0.774739</td>\n",
       "      <td>0.729753</td>\n",
       "      <td>0.664098</td>\n",
       "      <td>0.720883</td>\n",
       "      <td>0.713578</td>\n",
       "      <td>0.817927</td>\n",
       "      <td>0.771358</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.693212</td>\n",
       "      <td>2.821149</td>\n",
       "      <td>0.907480</td>\n",
       "      <td>0.853648</td>\n",
       "      <td>0.775297</td>\n",
       "      <td>0.730587</td>\n",
       "      <td>0.664475</td>\n",
       "      <td>0.722460</td>\n",
       "      <td>0.714967</td>\n",
       "      <td>0.818915</td>\n",
       "      <td>0.772370</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.690464</td>\n",
       "      <td>2.816503</td>\n",
       "      <td>0.906862</td>\n",
       "      <td>0.852447</td>\n",
       "      <td>0.775951</td>\n",
       "      <td>0.731977</td>\n",
       "      <td>0.664859</td>\n",
       "      <td>0.721541</td>\n",
       "      <td>0.714584</td>\n",
       "      <td>0.818942</td>\n",
       "      <td>0.772288</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "learner.fit_one_cycle(40, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.010076</td>\n",
       "      <td>4.460857</td>\n",
       "      <td>0.895116</td>\n",
       "      <td>0.826088</td>\n",
       "      <td>0.714836</td>\n",
       "      <td>0.666502</td>\n",
       "      <td>0.539149</td>\n",
       "      <td>0.526341</td>\n",
       "      <td>0.492704</td>\n",
       "      <td>0.778491</td>\n",
       "      <td>0.680107</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.693126</td>\n",
       "      <td>4.243952</td>\n",
       "      <td>0.901556</td>\n",
       "      <td>0.837522</td>\n",
       "      <td>0.741684</td>\n",
       "      <td>0.700417</td>\n",
       "      <td>0.567966</td>\n",
       "      <td>0.573374</td>\n",
       "      <td>0.541758</td>\n",
       "      <td>0.797802</td>\n",
       "      <td>0.707519</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.587859</td>\n",
       "      <td>4.134347</td>\n",
       "      <td>0.908098</td>\n",
       "      <td>0.843684</td>\n",
       "      <td>0.757421</td>\n",
       "      <td>0.719384</td>\n",
       "      <td>0.594488</td>\n",
       "      <td>0.613115</td>\n",
       "      <td>0.578301</td>\n",
       "      <td>0.809487</td>\n",
       "      <td>0.727630</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.499538</td>\n",
       "      <td>4.063585</td>\n",
       "      <td>0.912941</td>\n",
       "      <td>0.851460</td>\n",
       "      <td>0.769138</td>\n",
       "      <td>0.733175</td>\n",
       "      <td>0.605666</td>\n",
       "      <td>0.642911</td>\n",
       "      <td>0.600630</td>\n",
       "      <td>0.818915</td>\n",
       "      <td>0.741262</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.411286</td>\n",
       "      <td>4.011513</td>\n",
       "      <td>0.916237</td>\n",
       "      <td>0.857671</td>\n",
       "      <td>0.778179</td>\n",
       "      <td>0.742338</td>\n",
       "      <td>0.621092</td>\n",
       "      <td>0.665843</td>\n",
       "      <td>0.618764</td>\n",
       "      <td>0.825766</td>\n",
       "      <td>0.752638</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.341698</td>\n",
       "      <td>3.965773</td>\n",
       "      <td>0.917834</td>\n",
       "      <td>0.861011</td>\n",
       "      <td>0.786988</td>\n",
       "      <td>0.750805</td>\n",
       "      <td>0.638017</td>\n",
       "      <td>0.685466</td>\n",
       "      <td>0.644178</td>\n",
       "      <td>0.831214</td>\n",
       "      <td>0.763735</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.295968</td>\n",
       "      <td>3.934826</td>\n",
       "      <td>0.919998</td>\n",
       "      <td>0.863777</td>\n",
       "      <td>0.790943</td>\n",
       "      <td>0.757235</td>\n",
       "      <td>0.645115</td>\n",
       "      <td>0.697651</td>\n",
       "      <td>0.662250</td>\n",
       "      <td>0.834980</td>\n",
       "      <td>0.770592</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.252602</td>\n",
       "      <td>3.929391</td>\n",
       "      <td>0.919483</td>\n",
       "      <td>0.864408</td>\n",
       "      <td>0.787990</td>\n",
       "      <td>0.757593</td>\n",
       "      <td>0.646640</td>\n",
       "      <td>0.703007</td>\n",
       "      <td>0.668316</td>\n",
       "      <td>0.834366</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.201268</td>\n",
       "      <td>3.924004</td>\n",
       "      <td>0.918504</td>\n",
       "      <td>0.865711</td>\n",
       "      <td>0.789556</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.648773</td>\n",
       "      <td>0.706492</td>\n",
       "      <td>0.682552</td>\n",
       "      <td>0.835675</td>\n",
       "      <td>0.774856</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.175802</td>\n",
       "      <td>3.875288</td>\n",
       "      <td>0.923398</td>\n",
       "      <td>0.872652</td>\n",
       "      <td>0.800480</td>\n",
       "      <td>0.769078</td>\n",
       "      <td>0.660441</td>\n",
       "      <td>0.723590</td>\n",
       "      <td>0.702551</td>\n",
       "      <td>0.843327</td>\n",
       "      <td>0.785624</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.156728</td>\n",
       "      <td>3.888650</td>\n",
       "      <td>0.923449</td>\n",
       "      <td>0.869935</td>\n",
       "      <td>0.799540</td>\n",
       "      <td>0.769564</td>\n",
       "      <td>0.656572</td>\n",
       "      <td>0.723659</td>\n",
       "      <td>0.706958</td>\n",
       "      <td>0.842512</td>\n",
       "      <td>0.785015</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.117476</td>\n",
       "      <td>5.468695</td>\n",
       "      <td>0.922471</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.805387</td>\n",
       "      <td>0.769182</td>\n",
       "      <td>0.669952</td>\n",
       "      <td>0.735421</td>\n",
       "      <td>0.718194</td>\n",
       "      <td>0.844529</td>\n",
       "      <td>0.790958</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.099801</td>\n",
       "      <td>6.880422</td>\n",
       "      <td>0.921234</td>\n",
       "      <td>0.873491</td>\n",
       "      <td>0.802259</td>\n",
       "      <td>0.769045</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>0.730640</td>\n",
       "      <td>0.717504</td>\n",
       "      <td>0.843394</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.079859</td>\n",
       "      <td>4.385228</td>\n",
       "      <td>0.923862</td>\n",
       "      <td>0.872392</td>\n",
       "      <td>0.807698</td>\n",
       "      <td>0.773431</td>\n",
       "      <td>0.665959</td>\n",
       "      <td>0.736230</td>\n",
       "      <td>0.719960</td>\n",
       "      <td>0.846185</td>\n",
       "      <td>0.791666</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.029519</td>\n",
       "      <td>4.660513</td>\n",
       "      <td>0.924840</td>\n",
       "      <td>0.875003</td>\n",
       "      <td>0.809904</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.666932</td>\n",
       "      <td>0.735519</td>\n",
       "      <td>0.726799</td>\n",
       "      <td>0.847240</td>\n",
       "      <td>0.793107</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.034196</td>\n",
       "      <td>3.984916</td>\n",
       "      <td>0.924222</td>\n",
       "      <td>0.875631</td>\n",
       "      <td>0.809572</td>\n",
       "      <td>0.774835</td>\n",
       "      <td>0.668426</td>\n",
       "      <td>0.739966</td>\n",
       "      <td>0.731657</td>\n",
       "      <td>0.847894</td>\n",
       "      <td>0.794811</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.035892</td>\n",
       "      <td>3.858575</td>\n",
       "      <td>0.921904</td>\n",
       "      <td>0.873905</td>\n",
       "      <td>0.807623</td>\n",
       "      <td>0.774118</td>\n",
       "      <td>0.671785</td>\n",
       "      <td>0.734461</td>\n",
       "      <td>0.728035</td>\n",
       "      <td>0.846211</td>\n",
       "      <td>0.793205</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.985751</td>\n",
       "      <td>28.434931</td>\n",
       "      <td>0.924119</td>\n",
       "      <td>0.879805</td>\n",
       "      <td>0.812521</td>\n",
       "      <td>0.777492</td>\n",
       "      <td>0.672659</td>\n",
       "      <td>0.740416</td>\n",
       "      <td>0.729505</td>\n",
       "      <td>0.850298</td>\n",
       "      <td>0.796762</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.987145</td>\n",
       "      <td>104.945511</td>\n",
       "      <td>0.924892</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.812941</td>\n",
       "      <td>0.778589</td>\n",
       "      <td>0.674357</td>\n",
       "      <td>0.743300</td>\n",
       "      <td>0.734965</td>\n",
       "      <td>0.850792</td>\n",
       "      <td>0.798268</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.951705</td>\n",
       "      <td>23.664484</td>\n",
       "      <td>0.925458</td>\n",
       "      <td>0.880747</td>\n",
       "      <td>0.818036</td>\n",
       "      <td>0.780631</td>\n",
       "      <td>0.677468</td>\n",
       "      <td>0.748941</td>\n",
       "      <td>0.742573</td>\n",
       "      <td>0.852982</td>\n",
       "      <td>0.801610</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.942332</td>\n",
       "      <td>28.043577</td>\n",
       "      <td>0.925562</td>\n",
       "      <td>0.881061</td>\n",
       "      <td>0.814875</td>\n",
       "      <td>0.781589</td>\n",
       "      <td>0.677959</td>\n",
       "      <td>0.752795</td>\n",
       "      <td>0.734745</td>\n",
       "      <td>0.852555</td>\n",
       "      <td>0.801067</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.926552</td>\n",
       "      <td>74.764664</td>\n",
       "      <td>0.925510</td>\n",
       "      <td>0.881947</td>\n",
       "      <td>0.817796</td>\n",
       "      <td>0.783490</td>\n",
       "      <td>0.682704</td>\n",
       "      <td>0.755286</td>\n",
       "      <td>0.744044</td>\n",
       "      <td>0.853944</td>\n",
       "      <td>0.803948</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.913169</td>\n",
       "      <td>96.982124</td>\n",
       "      <td>0.924377</td>\n",
       "      <td>0.880642</td>\n",
       "      <td>0.816563</td>\n",
       "      <td>0.782103</td>\n",
       "      <td>0.679268</td>\n",
       "      <td>0.752994</td>\n",
       "      <td>0.745597</td>\n",
       "      <td>0.852662</td>\n",
       "      <td>0.802516</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.925209</td>\n",
       "      <td>4.788328</td>\n",
       "      <td>0.924686</td>\n",
       "      <td>0.882574</td>\n",
       "      <td>0.818163</td>\n",
       "      <td>0.782521</td>\n",
       "      <td>0.685328</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>0.747345</td>\n",
       "      <td>0.853730</td>\n",
       "      <td>0.804327</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.890488</td>\n",
       "      <td>34.387062</td>\n",
       "      <td>0.926180</td>\n",
       "      <td>0.880329</td>\n",
       "      <td>0.820484</td>\n",
       "      <td>0.782908</td>\n",
       "      <td>0.687232</td>\n",
       "      <td>0.755034</td>\n",
       "      <td>0.747666</td>\n",
       "      <td>0.854211</td>\n",
       "      <td>0.805150</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.880144</td>\n",
       "      <td>58.614761</td>\n",
       "      <td>0.926283</td>\n",
       "      <td>0.883359</td>\n",
       "      <td>0.820517</td>\n",
       "      <td>0.784738</td>\n",
       "      <td>0.684997</td>\n",
       "      <td>0.758894</td>\n",
       "      <td>0.750887</td>\n",
       "      <td>0.855466</td>\n",
       "      <td>0.806418</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.869138</td>\n",
       "      <td>50.756542</td>\n",
       "      <td>0.924686</td>\n",
       "      <td>0.882679</td>\n",
       "      <td>0.818622</td>\n",
       "      <td>0.784060</td>\n",
       "      <td>0.679534</td>\n",
       "      <td>0.754755</td>\n",
       "      <td>0.747506</td>\n",
       "      <td>0.854251</td>\n",
       "      <td>0.803964</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.861672</td>\n",
       "      <td>92.426422</td>\n",
       "      <td>0.925510</td>\n",
       "      <td>0.883516</td>\n",
       "      <td>0.820420</td>\n",
       "      <td>0.783293</td>\n",
       "      <td>0.684347</td>\n",
       "      <td>0.758026</td>\n",
       "      <td>0.750053</td>\n",
       "      <td>0.854932</td>\n",
       "      <td>0.805792</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.849501</td>\n",
       "      <td>105.903534</td>\n",
       "      <td>0.925716</td>\n",
       "      <td>0.884196</td>\n",
       "      <td>0.822389</td>\n",
       "      <td>0.786852</td>\n",
       "      <td>0.685489</td>\n",
       "      <td>0.758223</td>\n",
       "      <td>0.748577</td>\n",
       "      <td>0.856494</td>\n",
       "      <td>0.806796</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.816739</td>\n",
       "      <td>99.901886</td>\n",
       "      <td>0.926283</td>\n",
       "      <td>0.885971</td>\n",
       "      <td>0.822365</td>\n",
       "      <td>0.786192</td>\n",
       "      <td>0.685107</td>\n",
       "      <td>0.760654</td>\n",
       "      <td>0.754885</td>\n",
       "      <td>0.856935</td>\n",
       "      <td>0.807990</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.834143</td>\n",
       "      <td>113.533501</td>\n",
       "      <td>0.926025</td>\n",
       "      <td>0.884457</td>\n",
       "      <td>0.824776</td>\n",
       "      <td>0.788640</td>\n",
       "      <td>0.685814</td>\n",
       "      <td>0.762960</td>\n",
       "      <td>0.751423</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.808459</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.803639</td>\n",
       "      <td>104.016434</td>\n",
       "      <td>0.926180</td>\n",
       "      <td>0.884090</td>\n",
       "      <td>0.823954</td>\n",
       "      <td>0.787189</td>\n",
       "      <td>0.688491</td>\n",
       "      <td>0.757705</td>\n",
       "      <td>0.748195</td>\n",
       "      <td>0.857055</td>\n",
       "      <td>0.807488</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.816542</td>\n",
       "      <td>82.740440</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>0.885291</td>\n",
       "      <td>0.823911</td>\n",
       "      <td>0.787746</td>\n",
       "      <td>0.687182</td>\n",
       "      <td>0.760065</td>\n",
       "      <td>0.749894</td>\n",
       "      <td>0.857496</td>\n",
       "      <td>0.808039</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.798186</td>\n",
       "      <td>74.827637</td>\n",
       "      <td>0.925458</td>\n",
       "      <td>0.884457</td>\n",
       "      <td>0.823962</td>\n",
       "      <td>0.788320</td>\n",
       "      <td>0.687012</td>\n",
       "      <td>0.761370</td>\n",
       "      <td>0.753663</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>0.808418</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.785413</td>\n",
       "      <td>79.648552</td>\n",
       "      <td>0.926077</td>\n",
       "      <td>0.884560</td>\n",
       "      <td>0.823836</td>\n",
       "      <td>0.790367</td>\n",
       "      <td>0.686419</td>\n",
       "      <td>0.760650</td>\n",
       "      <td>0.751272</td>\n",
       "      <td>0.857883</td>\n",
       "      <td>0.808385</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.797284</td>\n",
       "      <td>78.817322</td>\n",
       "      <td>0.925974</td>\n",
       "      <td>0.884822</td>\n",
       "      <td>0.825136</td>\n",
       "      <td>0.789028</td>\n",
       "      <td>0.688653</td>\n",
       "      <td>0.760662</td>\n",
       "      <td>0.752127</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.808838</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.793829</td>\n",
       "      <td>68.457481</td>\n",
       "      <td>0.926643</td>\n",
       "      <td>0.885187</td>\n",
       "      <td>0.824713</td>\n",
       "      <td>0.790764</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.762495</td>\n",
       "      <td>0.752968</td>\n",
       "      <td>0.858497</td>\n",
       "      <td>0.809356</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.776735</td>\n",
       "      <td>71.499466</td>\n",
       "      <td>0.926849</td>\n",
       "      <td>0.884403</td>\n",
       "      <td>0.824062</td>\n",
       "      <td>0.788926</td>\n",
       "      <td>0.686690</td>\n",
       "      <td>0.761771</td>\n",
       "      <td>0.752663</td>\n",
       "      <td>0.857750</td>\n",
       "      <td>0.808632</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.820201</td>\n",
       "      <td>60.582264</td>\n",
       "      <td>0.925974</td>\n",
       "      <td>0.884559</td>\n",
       "      <td>0.823953</td>\n",
       "      <td>0.790537</td>\n",
       "      <td>0.687124</td>\n",
       "      <td>0.761835</td>\n",
       "      <td>0.750966</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.808632</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.788922</td>\n",
       "      <td>60.979694</td>\n",
       "      <td>0.926334</td>\n",
       "      <td>0.884821</td>\n",
       "      <td>0.824062</td>\n",
       "      <td>0.789483</td>\n",
       "      <td>0.687398</td>\n",
       "      <td>0.762360</td>\n",
       "      <td>0.751964</td>\n",
       "      <td>0.857856</td>\n",
       "      <td>0.808805</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(40, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor = cs.SingleInstruction()\n",
    "c_out = 16\n",
    "learner = cs.cs_learner(data, models.resnet18, instructor, td_c=c_out, pretrained=False, embedding=None,\n",
    "                        loss_func=loss, callback_fns=pose.Pckh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='466', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      21.24% [99/466 01:03<03:54 13.5258]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 6.31E-03\n",
      "Min loss divided by 10: 2.29E-01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.987977</td>\n",
       "      <td>4.567939</td>\n",
       "      <td>0.683495</td>\n",
       "      <td>0.478782</td>\n",
       "      <td>0.294244</td>\n",
       "      <td>0.254905</td>\n",
       "      <td>0.315764</td>\n",
       "      <td>0.286583</td>\n",
       "      <td>0.360475</td>\n",
       "      <td>0.432868</td>\n",
       "      <td>0.389050</td>\n",
       "      <td>06:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.549531</td>\n",
       "      <td>4.110280</td>\n",
       "      <td>0.762570</td>\n",
       "      <td>0.578335</td>\n",
       "      <td>0.399623</td>\n",
       "      <td>0.342505</td>\n",
       "      <td>0.380312</td>\n",
       "      <td>0.352551</td>\n",
       "      <td>0.424732</td>\n",
       "      <td>0.525720</td>\n",
       "      <td>0.471205</td>\n",
       "      <td>03:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.298253</td>\n",
       "      <td>3.896944</td>\n",
       "      <td>0.795642</td>\n",
       "      <td>0.643615</td>\n",
       "      <td>0.472875</td>\n",
       "      <td>0.403170</td>\n",
       "      <td>0.435949</td>\n",
       "      <td>0.412251</td>\n",
       "      <td>0.452081</td>\n",
       "      <td>0.583531</td>\n",
       "      <td>0.525675</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.134832</td>\n",
       "      <td>3.819671</td>\n",
       "      <td>0.803266</td>\n",
       "      <td>0.674983</td>\n",
       "      <td>0.506449</td>\n",
       "      <td>0.436856</td>\n",
       "      <td>0.434030</td>\n",
       "      <td>0.424291</td>\n",
       "      <td>0.455454</td>\n",
       "      <td>0.609853</td>\n",
       "      <td>0.543489</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.006347</td>\n",
       "      <td>3.763578</td>\n",
       "      <td>0.813414</td>\n",
       "      <td>0.678682</td>\n",
       "      <td>0.525978</td>\n",
       "      <td>0.466220</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.452192</td>\n",
       "      <td>0.488044</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>0.563493</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.894619</td>\n",
       "      <td>3.585719</td>\n",
       "      <td>0.842263</td>\n",
       "      <td>0.709830</td>\n",
       "      <td>0.560663</td>\n",
       "      <td>0.506625</td>\n",
       "      <td>0.500007</td>\n",
       "      <td>0.507147</td>\n",
       "      <td>0.511502</td>\n",
       "      <td>0.658903</td>\n",
       "      <td>0.600092</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.839085</td>\n",
       "      <td>3.490582</td>\n",
       "      <td>0.848650</td>\n",
       "      <td>0.723591</td>\n",
       "      <td>0.591043</td>\n",
       "      <td>0.533717</td>\n",
       "      <td>0.510717</td>\n",
       "      <td>0.514413</td>\n",
       "      <td>0.528187</td>\n",
       "      <td>0.678053</td>\n",
       "      <td>0.616202</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.766068</td>\n",
       "      <td>3.484401</td>\n",
       "      <td>0.852411</td>\n",
       "      <td>0.739665</td>\n",
       "      <td>0.599669</td>\n",
       "      <td>0.542187</td>\n",
       "      <td>0.521070</td>\n",
       "      <td>0.526004</td>\n",
       "      <td>0.542382</td>\n",
       "      <td>0.687321</td>\n",
       "      <td>0.626451</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.724278</td>\n",
       "      <td>3.485705</td>\n",
       "      <td>0.838553</td>\n",
       "      <td>0.716271</td>\n",
       "      <td>0.576741</td>\n",
       "      <td>0.536975</td>\n",
       "      <td>0.523268</td>\n",
       "      <td>0.539313</td>\n",
       "      <td>0.554926</td>\n",
       "      <td>0.670775</td>\n",
       "      <td>0.619602</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.646555</td>\n",
       "      <td>3.395620</td>\n",
       "      <td>0.853699</td>\n",
       "      <td>0.730408</td>\n",
       "      <td>0.607502</td>\n",
       "      <td>0.572667</td>\n",
       "      <td>0.535071</td>\n",
       "      <td>0.551279</td>\n",
       "      <td>0.546382</td>\n",
       "      <td>0.694479</td>\n",
       "      <td>0.636568</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.606456</td>\n",
       "      <td>3.332185</td>\n",
       "      <td>0.851741</td>\n",
       "      <td>0.748639</td>\n",
       "      <td>0.618995</td>\n",
       "      <td>0.587533</td>\n",
       "      <td>0.533625</td>\n",
       "      <td>0.551880</td>\n",
       "      <td>0.571670</td>\n",
       "      <td>0.705003</td>\n",
       "      <td>0.645631</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.542462</td>\n",
       "      <td>3.301651</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.763644</td>\n",
       "      <td>0.641701</td>\n",
       "      <td>0.590830</td>\n",
       "      <td>0.554826</td>\n",
       "      <td>0.578905</td>\n",
       "      <td>0.571550</td>\n",
       "      <td>0.717716</td>\n",
       "      <td>0.660045</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.513818</td>\n",
       "      <td>3.238717</td>\n",
       "      <td>0.868793</td>\n",
       "      <td>0.768184</td>\n",
       "      <td>0.655046</td>\n",
       "      <td>0.596326</td>\n",
       "      <td>0.570760</td>\n",
       "      <td>0.590691</td>\n",
       "      <td>0.595852</td>\n",
       "      <td>0.725368</td>\n",
       "      <td>0.671257</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.432246</td>\n",
       "      <td>3.168735</td>\n",
       "      <td>0.876108</td>\n",
       "      <td>0.786402</td>\n",
       "      <td>0.674539</td>\n",
       "      <td>0.627831</td>\n",
       "      <td>0.590097</td>\n",
       "      <td>0.611013</td>\n",
       "      <td>0.599646</td>\n",
       "      <td>0.744264</td>\n",
       "      <td>0.688783</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.384914</td>\n",
       "      <td>3.192070</td>\n",
       "      <td>0.877705</td>\n",
       "      <td>0.770993</td>\n",
       "      <td>0.667165</td>\n",
       "      <td>0.627823</td>\n",
       "      <td>0.579576</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>0.600442</td>\n",
       "      <td>0.738923</td>\n",
       "      <td>0.684247</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.383592</td>\n",
       "      <td>3.123953</td>\n",
       "      <td>0.878323</td>\n",
       "      <td>0.792816</td>\n",
       "      <td>0.686977</td>\n",
       "      <td>0.637424</td>\n",
       "      <td>0.600864</td>\n",
       "      <td>0.631889</td>\n",
       "      <td>0.611873</td>\n",
       "      <td>0.751810</td>\n",
       "      <td>0.698991</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.370780</td>\n",
       "      <td>3.105838</td>\n",
       "      <td>0.878529</td>\n",
       "      <td>0.784631</td>\n",
       "      <td>0.682522</td>\n",
       "      <td>0.631622</td>\n",
       "      <td>0.593233</td>\n",
       "      <td>0.626880</td>\n",
       "      <td>0.613242</td>\n",
       "      <td>0.747322</td>\n",
       "      <td>0.694587</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.314688</td>\n",
       "      <td>3.099740</td>\n",
       "      <td>0.886462</td>\n",
       "      <td>0.795952</td>\n",
       "      <td>0.690253</td>\n",
       "      <td>0.643025</td>\n",
       "      <td>0.600428</td>\n",
       "      <td>0.640730</td>\n",
       "      <td>0.630147</td>\n",
       "      <td>0.756924</td>\n",
       "      <td>0.705148</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.269376</td>\n",
       "      <td>3.062152</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.803990</td>\n",
       "      <td>0.701975</td>\n",
       "      <td>0.652010</td>\n",
       "      <td>0.610967</td>\n",
       "      <td>0.643652</td>\n",
       "      <td>0.636072</td>\n",
       "      <td>0.763227</td>\n",
       "      <td>0.711627</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.254081</td>\n",
       "      <td>3.056257</td>\n",
       "      <td>0.888574</td>\n",
       "      <td>0.807543</td>\n",
       "      <td>0.708427</td>\n",
       "      <td>0.646175</td>\n",
       "      <td>0.610907</td>\n",
       "      <td>0.637526</td>\n",
       "      <td>0.631078</td>\n",
       "      <td>0.765605</td>\n",
       "      <td>0.711775</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.204851</td>\n",
       "      <td>2.988866</td>\n",
       "      <td>0.890068</td>\n",
       "      <td>0.816581</td>\n",
       "      <td>0.721525</td>\n",
       "      <td>0.675473</td>\n",
       "      <td>0.624172</td>\n",
       "      <td>0.658235</td>\n",
       "      <td>0.660408</td>\n",
       "      <td>0.778532</td>\n",
       "      <td>0.727482</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.182095</td>\n",
       "      <td>2.993324</td>\n",
       "      <td>0.889707</td>\n",
       "      <td>0.814335</td>\n",
       "      <td>0.720566</td>\n",
       "      <td>0.672342</td>\n",
       "      <td>0.617502</td>\n",
       "      <td>0.661559</td>\n",
       "      <td>0.657345</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.725563</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.150267</td>\n",
       "      <td>2.980081</td>\n",
       "      <td>0.892180</td>\n",
       "      <td>0.822376</td>\n",
       "      <td>0.720322</td>\n",
       "      <td>0.676034</td>\n",
       "      <td>0.621934</td>\n",
       "      <td>0.661622</td>\n",
       "      <td>0.665270</td>\n",
       "      <td>0.780415</td>\n",
       "      <td>0.729251</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.112615</td>\n",
       "      <td>2.980723</td>\n",
       "      <td>0.896147</td>\n",
       "      <td>0.813596</td>\n",
       "      <td>0.725976</td>\n",
       "      <td>0.677777</td>\n",
       "      <td>0.622844</td>\n",
       "      <td>0.670313</td>\n",
       "      <td>0.669041</td>\n",
       "      <td>0.780989</td>\n",
       "      <td>0.731243</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.086838</td>\n",
       "      <td>2.923215</td>\n",
       "      <td>0.898568</td>\n",
       "      <td>0.828064</td>\n",
       "      <td>0.732381</td>\n",
       "      <td>0.693571</td>\n",
       "      <td>0.625754</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.667590</td>\n",
       "      <td>0.790697</td>\n",
       "      <td>0.737253</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.026348</td>\n",
       "      <td>2.934602</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.829895</td>\n",
       "      <td>0.740640</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.637357</td>\n",
       "      <td>0.684940</td>\n",
       "      <td>0.681519</td>\n",
       "      <td>0.792741</td>\n",
       "      <td>0.743847</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.015938</td>\n",
       "      <td>2.905072</td>\n",
       "      <td>0.894704</td>\n",
       "      <td>0.834231</td>\n",
       "      <td>0.743809</td>\n",
       "      <td>0.699212</td>\n",
       "      <td>0.638730</td>\n",
       "      <td>0.682327</td>\n",
       "      <td>0.675397</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>0.744736</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.979323</td>\n",
       "      <td>2.888432</td>\n",
       "      <td>0.901041</td>\n",
       "      <td>0.836103</td>\n",
       "      <td>0.749427</td>\n",
       "      <td>0.704853</td>\n",
       "      <td>0.649327</td>\n",
       "      <td>0.692205</td>\n",
       "      <td>0.680129</td>\n",
       "      <td>0.800259</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.912364</td>\n",
       "      <td>2.862585</td>\n",
       "      <td>0.902071</td>\n",
       "      <td>0.839392</td>\n",
       "      <td>0.754264</td>\n",
       "      <td>0.706566</td>\n",
       "      <td>0.649705</td>\n",
       "      <td>0.698259</td>\n",
       "      <td>0.703042</td>\n",
       "      <td>0.802983</td>\n",
       "      <td>0.755997</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.884891</td>\n",
       "      <td>2.852266</td>\n",
       "      <td>0.902071</td>\n",
       "      <td>0.844772</td>\n",
       "      <td>0.762089</td>\n",
       "      <td>0.715994</td>\n",
       "      <td>0.655426</td>\n",
       "      <td>0.708785</td>\n",
       "      <td>0.708131</td>\n",
       "      <td>0.808539</td>\n",
       "      <td>0.762154</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.879240</td>\n",
       "      <td>2.879434</td>\n",
       "      <td>0.901504</td>\n",
       "      <td>0.841642</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.713928</td>\n",
       "      <td>0.645986</td>\n",
       "      <td>0.697995</td>\n",
       "      <td>0.692988</td>\n",
       "      <td>0.804573</td>\n",
       "      <td>0.755305</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.828411</td>\n",
       "      <td>2.823664</td>\n",
       "      <td>0.906501</td>\n",
       "      <td>0.849161</td>\n",
       "      <td>0.767705</td>\n",
       "      <td>0.726929</td>\n",
       "      <td>0.661652</td>\n",
       "      <td>0.710892</td>\n",
       "      <td>0.714973</td>\n",
       "      <td>0.814802</td>\n",
       "      <td>0.767950</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.805354</td>\n",
       "      <td>2.819078</td>\n",
       "      <td>0.905368</td>\n",
       "      <td>0.852343</td>\n",
       "      <td>0.769865</td>\n",
       "      <td>0.724451</td>\n",
       "      <td>0.663506</td>\n",
       "      <td>0.714178</td>\n",
       "      <td>0.711661</td>\n",
       "      <td>0.815256</td>\n",
       "      <td>0.768567</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.754168</td>\n",
       "      <td>2.821755</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.852239</td>\n",
       "      <td>0.770572</td>\n",
       "      <td>0.730738</td>\n",
       "      <td>0.665084</td>\n",
       "      <td>0.717303</td>\n",
       "      <td>0.717261</td>\n",
       "      <td>0.817446</td>\n",
       "      <td>0.771152</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.742917</td>\n",
       "      <td>2.819952</td>\n",
       "      <td>0.907068</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.772681</td>\n",
       "      <td>0.731361</td>\n",
       "      <td>0.663942</td>\n",
       "      <td>0.717491</td>\n",
       "      <td>0.718201</td>\n",
       "      <td>0.817780</td>\n",
       "      <td>0.771308</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.730669</td>\n",
       "      <td>2.812835</td>\n",
       "      <td>0.907944</td>\n",
       "      <td>0.854224</td>\n",
       "      <td>0.771916</td>\n",
       "      <td>0.731497</td>\n",
       "      <td>0.663725</td>\n",
       "      <td>0.723193</td>\n",
       "      <td>0.721434</td>\n",
       "      <td>0.818608</td>\n",
       "      <td>0.772848</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.710901</td>\n",
       "      <td>2.816504</td>\n",
       "      <td>0.907325</td>\n",
       "      <td>0.853338</td>\n",
       "      <td>0.773420</td>\n",
       "      <td>0.731584</td>\n",
       "      <td>0.664103</td>\n",
       "      <td>0.722795</td>\n",
       "      <td>0.720421</td>\n",
       "      <td>0.818595</td>\n",
       "      <td>0.772741</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.696899</td>\n",
       "      <td>2.819534</td>\n",
       "      <td>0.907480</td>\n",
       "      <td>0.853597</td>\n",
       "      <td>0.775868</td>\n",
       "      <td>0.733074</td>\n",
       "      <td>0.663175</td>\n",
       "      <td>0.722141</td>\n",
       "      <td>0.719808</td>\n",
       "      <td>0.819663</td>\n",
       "      <td>0.773111</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.694638</td>\n",
       "      <td>2.815181</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.854331</td>\n",
       "      <td>0.775931</td>\n",
       "      <td>0.731788</td>\n",
       "      <td>0.662959</td>\n",
       "      <td>0.723518</td>\n",
       "      <td>0.720966</td>\n",
       "      <td>0.819436</td>\n",
       "      <td>0.773235</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.688393</td>\n",
       "      <td>2.816675</td>\n",
       "      <td>0.906965</td>\n",
       "      <td>0.853442</td>\n",
       "      <td>0.774778</td>\n",
       "      <td>0.731115</td>\n",
       "      <td>0.663398</td>\n",
       "      <td>0.723001</td>\n",
       "      <td>0.719889</td>\n",
       "      <td>0.818755</td>\n",
       "      <td>0.772700</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(40, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('baselinev2-noinst-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (29866 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (8/16) (128, 128),Pose (7/16) (128, 128),Pose (6/16) (128, 128),Pose (14/16) (128, 128),Pose (1/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: PoseItemList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: PoseLabelList\n",
       "Pose (14/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128),Pose (16/16) (128, 128),Pose (10/16) (128, 128)\n",
       "Path: /home/labs/waic/omrik/LIP;\n",
       "\n",
       "Test: None, model=CounterStream(\n",
       "  (bu_body): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (td): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (upsample): Sequential(\n",
       "          (0): Lambda()\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): TDBasicBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TDHead(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (laterals): ModuleList(\n",
       "    (0): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (12): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (13): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (14): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (15): Lateral(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function loss at 0x7f265b05b730>, metrics=[], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/labs/waic/omrik/LIP'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'pose.Pckh'>], callbacks=[SingleInstruction], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Lambda()\n",
       "  (17): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (28): ReLU(inplace=True)\n",
       "  (29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (30): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (33): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (34): ReLU(inplace=True)\n",
       "  (35): Lambda()\n",
       "  (36): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (39): ReLU(inplace=True)\n",
       "  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (42): ReLU(inplace=True)\n",
       "  (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (45): ReLU(inplace=True)\n",
       "  (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (49): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Lambda()\n",
       "  (55): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (56): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (58): ReLU(inplace=True)\n",
       "  (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (61): ReLU(inplace=True)\n",
       "  (62): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (64): ReLU(inplace=True)\n",
       "  (65): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (66): ReLU(inplace=True)\n",
       "  (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (69): ReLU(inplace=True)\n",
       "  (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (72): ReLU(inplace=True)\n",
       "  (73): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (76): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (77): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (78): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (79): ReLU(inplace=True)\n",
       "  (80): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (81): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (82): ReLU(inplace=True)\n",
       "  (83): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (84): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (85): ReLU(inplace=True)\n",
       "  (86): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (87): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (88): ReLU(inplace=True)\n",
       "  (89): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (90): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (91): ReLU(inplace=True)\n",
       "  (92): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (93): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (94): ReLU(inplace=True)\n",
       "  (95): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (96): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (97): ReLU(inplace=True)\n",
       "  (98): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (99): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (100): ReLU(inplace=True)\n",
       "  (101): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (102): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (103): ReLU(inplace=True)\n",
       "  (104): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (105): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (106): ReLU(inplace=True)\n",
       "  (107): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (108): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (109): ReLU(inplace=True)\n",
       "  (110): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (111): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (112): ReLU(inplace=True)\n",
       "  (113): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (114): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (115): ReLU(inplace=True)\n",
       "  (116): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (117): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (118): ReLU(inplace=True)\n",
       "  (119): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (120): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (121): ReLU(inplace=True)\n",
       "  (122): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (123): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (124): ReLU(inplace=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('baselinev2-noinst-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='466', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      21.24% [99/466 01:35<05:55 16.4634]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.20E-03\n",
      "Min loss divided by 10: 1.10E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.data = pose.get_data(root, 256)\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>Head</th>\n",
       "      <th>Shoulder</th>\n",
       "      <th>Elbow</th>\n",
       "      <th>Wrist</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>UBody</th>\n",
       "      <th>Total</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.941670</td>\n",
       "      <td>4.420929</td>\n",
       "      <td>0.892026</td>\n",
       "      <td>0.821754</td>\n",
       "      <td>0.722644</td>\n",
       "      <td>0.671100</td>\n",
       "      <td>0.546777</td>\n",
       "      <td>0.537996</td>\n",
       "      <td>0.492197</td>\n",
       "      <td>0.779613</td>\n",
       "      <td>0.683367</td>\n",
       "      <td>03:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.679213</td>\n",
       "      <td>4.230792</td>\n",
       "      <td>0.901504</td>\n",
       "      <td>0.838984</td>\n",
       "      <td>0.745959</td>\n",
       "      <td>0.703098</td>\n",
       "      <td>0.572372</td>\n",
       "      <td>0.588496</td>\n",
       "      <td>0.539368</td>\n",
       "      <td>0.799872</td>\n",
       "      <td>0.711108</td>\n",
       "      <td>03:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.542892</td>\n",
       "      <td>4.126417</td>\n",
       "      <td>0.905934</td>\n",
       "      <td>0.845043</td>\n",
       "      <td>0.759313</td>\n",
       "      <td>0.725105</td>\n",
       "      <td>0.595015</td>\n",
       "      <td>0.622614</td>\n",
       "      <td>0.570308</td>\n",
       "      <td>0.811103</td>\n",
       "      <td>0.729046</td>\n",
       "      <td>02:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.457860</td>\n",
       "      <td>4.054231</td>\n",
       "      <td>0.912528</td>\n",
       "      <td>0.851726</td>\n",
       "      <td>0.768679</td>\n",
       "      <td>0.735720</td>\n",
       "      <td>0.610613</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>0.604391</td>\n",
       "      <td>0.819356</td>\n",
       "      <td>0.743081</td>\n",
       "      <td>02:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.401666</td>\n",
       "      <td>3.998146</td>\n",
       "      <td>0.916237</td>\n",
       "      <td>0.856684</td>\n",
       "      <td>0.777229</td>\n",
       "      <td>0.745759</td>\n",
       "      <td>0.627315</td>\n",
       "      <td>0.670893</td>\n",
       "      <td>0.628233</td>\n",
       "      <td>0.826086</td>\n",
       "      <td>0.755421</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.344970</td>\n",
       "      <td>3.958867</td>\n",
       "      <td>0.919019</td>\n",
       "      <td>0.861594</td>\n",
       "      <td>0.782414</td>\n",
       "      <td>0.750810</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.686992</td>\n",
       "      <td>0.648472</td>\n",
       "      <td>0.830547</td>\n",
       "      <td>0.763472</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.259744</td>\n",
       "      <td>3.924481</td>\n",
       "      <td>0.919380</td>\n",
       "      <td>0.864202</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.758517</td>\n",
       "      <td>0.644500</td>\n",
       "      <td>0.698910</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.835074</td>\n",
       "      <td>0.770913</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.221149</td>\n",
       "      <td>3.915584</td>\n",
       "      <td>0.918401</td>\n",
       "      <td>0.864149</td>\n",
       "      <td>0.790968</td>\n",
       "      <td>0.760184</td>\n",
       "      <td>0.650776</td>\n",
       "      <td>0.712075</td>\n",
       "      <td>0.685247</td>\n",
       "      <td>0.835381</td>\n",
       "      <td>0.775968</td>\n",
       "      <td>02:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.202864</td>\n",
       "      <td>3.888808</td>\n",
       "      <td>0.922265</td>\n",
       "      <td>0.867751</td>\n",
       "      <td>0.797939</td>\n",
       "      <td>0.765574</td>\n",
       "      <td>0.657716</td>\n",
       "      <td>0.719200</td>\n",
       "      <td>0.692955</td>\n",
       "      <td>0.840309</td>\n",
       "      <td>0.781771</td>\n",
       "      <td>02:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.167010</td>\n",
       "      <td>3.866821</td>\n",
       "      <td>0.921080</td>\n",
       "      <td>0.869472</td>\n",
       "      <td>0.801635</td>\n",
       "      <td>0.769563</td>\n",
       "      <td>0.664977</td>\n",
       "      <td>0.726153</td>\n",
       "      <td>0.703807</td>\n",
       "      <td>0.842312</td>\n",
       "      <td>0.786134</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.119502</td>\n",
       "      <td>3.861969</td>\n",
       "      <td>0.922625</td>\n",
       "      <td>0.867959</td>\n",
       "      <td>0.801185</td>\n",
       "      <td>0.767038</td>\n",
       "      <td>0.666881</td>\n",
       "      <td>0.728237</td>\n",
       "      <td>0.714350</td>\n",
       "      <td>0.841617</td>\n",
       "      <td>0.787385</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.121097</td>\n",
       "      <td>3.842319</td>\n",
       "      <td>0.924943</td>\n",
       "      <td>0.870885</td>\n",
       "      <td>0.803397</td>\n",
       "      <td>0.773834</td>\n",
       "      <td>0.669005</td>\n",
       "      <td>0.735905</td>\n",
       "      <td>0.717814</td>\n",
       "      <td>0.845116</td>\n",
       "      <td>0.791197</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.081400</td>\n",
       "      <td>3.848563</td>\n",
       "      <td>0.922677</td>\n",
       "      <td>0.873230</td>\n",
       "      <td>0.805797</td>\n",
       "      <td>0.775446</td>\n",
       "      <td>0.675549</td>\n",
       "      <td>0.734941</td>\n",
       "      <td>0.724349</td>\n",
       "      <td>0.846105</td>\n",
       "      <td>0.793370</td>\n",
       "      <td>01:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.064818</td>\n",
       "      <td>3.832309</td>\n",
       "      <td>0.923501</td>\n",
       "      <td>0.873808</td>\n",
       "      <td>0.807917</td>\n",
       "      <td>0.777727</td>\n",
       "      <td>0.672268</td>\n",
       "      <td>0.739845</td>\n",
       "      <td>0.730795</td>\n",
       "      <td>0.847533</td>\n",
       "      <td>0.795066</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.036694</td>\n",
       "      <td>3.837006</td>\n",
       "      <td>0.921543</td>\n",
       "      <td>0.872349</td>\n",
       "      <td>0.807630</td>\n",
       "      <td>0.775006</td>\n",
       "      <td>0.678446</td>\n",
       "      <td>0.746720</td>\n",
       "      <td>0.727594</td>\n",
       "      <td>0.845931</td>\n",
       "      <td>0.795527</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.028177</td>\n",
       "      <td>3.816095</td>\n",
       "      <td>0.925510</td>\n",
       "      <td>0.877046</td>\n",
       "      <td>0.813563</td>\n",
       "      <td>0.781198</td>\n",
       "      <td>0.681336</td>\n",
       "      <td>0.747499</td>\n",
       "      <td>0.732096</td>\n",
       "      <td>0.851099</td>\n",
       "      <td>0.799733</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.999719</td>\n",
       "      <td>3.825934</td>\n",
       "      <td>0.926386</td>\n",
       "      <td>0.874590</td>\n",
       "      <td>0.812493</td>\n",
       "      <td>0.777064</td>\n",
       "      <td>0.675552</td>\n",
       "      <td>0.745068</td>\n",
       "      <td>0.732885</td>\n",
       "      <td>0.849443</td>\n",
       "      <td>0.797618</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.993608</td>\n",
       "      <td>3.819024</td>\n",
       "      <td>0.925252</td>\n",
       "      <td>0.878248</td>\n",
       "      <td>0.815373</td>\n",
       "      <td>0.781526</td>\n",
       "      <td>0.677461</td>\n",
       "      <td>0.752674</td>\n",
       "      <td>0.736959</td>\n",
       "      <td>0.851860</td>\n",
       "      <td>0.800787</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.960296</td>\n",
       "      <td>3.820272</td>\n",
       "      <td>0.923656</td>\n",
       "      <td>0.876312</td>\n",
       "      <td>0.817325</td>\n",
       "      <td>0.780518</td>\n",
       "      <td>0.683575</td>\n",
       "      <td>0.752715</td>\n",
       "      <td>0.744411</td>\n",
       "      <td>0.851193</td>\n",
       "      <td>0.802104</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.940506</td>\n",
       "      <td>3.808889</td>\n",
       "      <td>0.924480</td>\n",
       "      <td>0.877461</td>\n",
       "      <td>0.818262</td>\n",
       "      <td>0.783003</td>\n",
       "      <td>0.681618</td>\n",
       "      <td>0.755356</td>\n",
       "      <td>0.742561</td>\n",
       "      <td>0.852528</td>\n",
       "      <td>0.802763</td>\n",
       "      <td>01:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.962443</td>\n",
       "      <td>3.805321</td>\n",
       "      <td>0.925407</td>\n",
       "      <td>0.877565</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.784771</td>\n",
       "      <td>0.685646</td>\n",
       "      <td>0.751032</td>\n",
       "      <td>0.745266</td>\n",
       "      <td>0.852902</td>\n",
       "      <td>0.803347</td>\n",
       "      <td>01:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.921041</td>\n",
       "      <td>3.802328</td>\n",
       "      <td>0.926128</td>\n",
       "      <td>0.879911</td>\n",
       "      <td>0.818618</td>\n",
       "      <td>0.785308</td>\n",
       "      <td>0.685751</td>\n",
       "      <td>0.753323</td>\n",
       "      <td>0.747186</td>\n",
       "      <td>0.854224</td>\n",
       "      <td>0.804672</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.887917</td>\n",
       "      <td>3.813856</td>\n",
       "      <td>0.926077</td>\n",
       "      <td>0.878452</td>\n",
       "      <td>0.818602</td>\n",
       "      <td>0.785420</td>\n",
       "      <td>0.684603</td>\n",
       "      <td>0.755684</td>\n",
       "      <td>0.746033</td>\n",
       "      <td>0.853850</td>\n",
       "      <td>0.804442</td>\n",
       "      <td>01:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.879297</td>\n",
       "      <td>3.816488</td>\n",
       "      <td>0.924789</td>\n",
       "      <td>0.880699</td>\n",
       "      <td>0.817121</td>\n",
       "      <td>0.785983</td>\n",
       "      <td>0.684265</td>\n",
       "      <td>0.756914</td>\n",
       "      <td>0.743433</td>\n",
       "      <td>0.853863</td>\n",
       "      <td>0.804277</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.871018</td>\n",
       "      <td>3.802056</td>\n",
       "      <td>0.927261</td>\n",
       "      <td>0.882316</td>\n",
       "      <td>0.822399</td>\n",
       "      <td>0.787717</td>\n",
       "      <td>0.686998</td>\n",
       "      <td>0.756385</td>\n",
       "      <td>0.749356</td>\n",
       "      <td>0.856628</td>\n",
       "      <td>0.806961</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.841858</td>\n",
       "      <td>3.807220</td>\n",
       "      <td>0.926849</td>\n",
       "      <td>0.881009</td>\n",
       "      <td>0.822200</td>\n",
       "      <td>0.788555</td>\n",
       "      <td>0.686731</td>\n",
       "      <td>0.756650</td>\n",
       "      <td>0.746429</td>\n",
       "      <td>0.856334</td>\n",
       "      <td>0.806459</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.866856</td>\n",
       "      <td>3.807452</td>\n",
       "      <td>0.926643</td>\n",
       "      <td>0.880802</td>\n",
       "      <td>0.823432</td>\n",
       "      <td>0.787661</td>\n",
       "      <td>0.685583</td>\n",
       "      <td>0.758568</td>\n",
       "      <td>0.749204</td>\n",
       "      <td>0.856321</td>\n",
       "      <td>0.806813</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.840550</td>\n",
       "      <td>3.811844</td>\n",
       "      <td>0.925407</td>\n",
       "      <td>0.881009</td>\n",
       "      <td>0.822909</td>\n",
       "      <td>0.787447</td>\n",
       "      <td>0.688038</td>\n",
       "      <td>0.755623</td>\n",
       "      <td>0.746036</td>\n",
       "      <td>0.855867</td>\n",
       "      <td>0.806195</td>\n",
       "      <td>02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.836638</td>\n",
       "      <td>3.807049</td>\n",
       "      <td>0.924943</td>\n",
       "      <td>0.882314</td>\n",
       "      <td>0.822611</td>\n",
       "      <td>0.788409</td>\n",
       "      <td>0.688526</td>\n",
       "      <td>0.758681</td>\n",
       "      <td>0.748569</td>\n",
       "      <td>0.856241</td>\n",
       "      <td>0.807158</td>\n",
       "      <td>02:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.813649</td>\n",
       "      <td>3.813834</td>\n",
       "      <td>0.924377</td>\n",
       "      <td>0.881585</td>\n",
       "      <td>0.822018</td>\n",
       "      <td>0.790352</td>\n",
       "      <td>0.688360</td>\n",
       "      <td>0.756459</td>\n",
       "      <td>0.747655</td>\n",
       "      <td>0.856227</td>\n",
       "      <td>0.806747</td>\n",
       "      <td>02:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.791735</td>\n",
       "      <td>3.808593</td>\n",
       "      <td>0.926180</td>\n",
       "      <td>0.883360</td>\n",
       "      <td>0.825881</td>\n",
       "      <td>0.788805</td>\n",
       "      <td>0.688193</td>\n",
       "      <td>0.759464</td>\n",
       "      <td>0.754502</td>\n",
       "      <td>0.857723</td>\n",
       "      <td>0.808756</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.810657</td>\n",
       "      <td>3.810932</td>\n",
       "      <td>0.926025</td>\n",
       "      <td>0.884143</td>\n",
       "      <td>0.822661</td>\n",
       "      <td>0.788773</td>\n",
       "      <td>0.688796</td>\n",
       "      <td>0.759742</td>\n",
       "      <td>0.756114</td>\n",
       "      <td>0.857095</td>\n",
       "      <td>0.808665</td>\n",
       "      <td>02:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.778644</td>\n",
       "      <td>3.809788</td>\n",
       "      <td>0.926489</td>\n",
       "      <td>0.883989</td>\n",
       "      <td>0.824829</td>\n",
       "      <td>0.789860</td>\n",
       "      <td>0.688639</td>\n",
       "      <td>0.762152</td>\n",
       "      <td>0.756109</td>\n",
       "      <td>0.857963</td>\n",
       "      <td>0.809480</td>\n",
       "      <td>01:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.781223</td>\n",
       "      <td>3.812295</td>\n",
       "      <td>0.927210</td>\n",
       "      <td>0.884248</td>\n",
       "      <td>0.824364</td>\n",
       "      <td>0.790230</td>\n",
       "      <td>0.690216</td>\n",
       "      <td>0.761437</td>\n",
       "      <td>0.755354</td>\n",
       "      <td>0.858190</td>\n",
       "      <td>0.809686</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.772576</td>\n",
       "      <td>3.814741</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>0.883569</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.790024</td>\n",
       "      <td>0.690377</td>\n",
       "      <td>0.763010</td>\n",
       "      <td>0.756964</td>\n",
       "      <td>0.857790</td>\n",
       "      <td>0.809834</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.772427</td>\n",
       "      <td>3.813795</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>0.884771</td>\n",
       "      <td>0.825504</td>\n",
       "      <td>0.790842</td>\n",
       "      <td>0.691415</td>\n",
       "      <td>0.762550</td>\n",
       "      <td>0.756118</td>\n",
       "      <td>0.858497</td>\n",
       "      <td>0.810278</td>\n",
       "      <td>02:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.775257</td>\n",
       "      <td>3.819926</td>\n",
       "      <td>0.926077</td>\n",
       "      <td>0.884092</td>\n",
       "      <td>0.825998</td>\n",
       "      <td>0.791307</td>\n",
       "      <td>0.690267</td>\n",
       "      <td>0.760519</td>\n",
       "      <td>0.755968</td>\n",
       "      <td>0.858511</td>\n",
       "      <td>0.809842</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.777512</td>\n",
       "      <td>3.815022</td>\n",
       "      <td>0.926849</td>\n",
       "      <td>0.884613</td>\n",
       "      <td>0.825392</td>\n",
       "      <td>0.789901</td>\n",
       "      <td>0.690539</td>\n",
       "      <td>0.761826</td>\n",
       "      <td>0.756503</td>\n",
       "      <td>0.858364</td>\n",
       "      <td>0.810015</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.781475</td>\n",
       "      <td>3.814603</td>\n",
       "      <td>0.927210</td>\n",
       "      <td>0.884561</td>\n",
       "      <td>0.825828</td>\n",
       "      <td>0.790851</td>\n",
       "      <td>0.690707</td>\n",
       "      <td>0.760652</td>\n",
       "      <td>0.756045</td>\n",
       "      <td>0.858778</td>\n",
       "      <td>0.810097</td>\n",
       "      <td>01:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.785566</td>\n",
       "      <td>3.816266</td>\n",
       "      <td>0.926180</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.790959</td>\n",
       "      <td>0.690157</td>\n",
       "      <td>0.762620</td>\n",
       "      <td>0.756042</td>\n",
       "      <td>0.858471</td>\n",
       "      <td>0.810073</td>\n",
       "      <td>01:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(40, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
